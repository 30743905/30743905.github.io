<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[并发编程锁之synchronized总结]]></title>
    <url>%2F2018%2F02%2F07%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E9%94%81%E4%B9%8Bsynchronized%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[锁synchronized并发编程中数据同步需要依赖锁进行控制，上篇博文通过ReentrantLock源码分析也对Lock实现锁机制的大致原理有了一个了解，Lock主要是通过编码的方式实现锁，其核心就是：CAS+循环，CAS原子操作需要依赖底层硬件层特殊的CPU指令。这节我们来看下Java中另一种非常常见的实现同步的方式：synchronized。synchronized主要通过底层JVM进行实现，而且JVM为了优化，产生偏向锁、轻量级锁、重量级锁，由于其处于JVM底层实现中，对很多并发编程人员来说能清晰理解它们间的区别还是件困难的事。通过本篇博文，构建出对Java中锁得体系结构，让你对其有个更系统全面的认知。 synchronized实现同步主要分为两种情况：​ 1、同步方法：synchronized方法则会被翻译成普通的方法调用，在JVM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的access_flags字段中的synchronized标志位设置成1，表示该方法是同步方法，当某个线程要访问某个方法的时候，使用调用该方法的对象(普通方法同步)或该方法所属的Class在JVM的内部对象表示Klass做为监视器锁(静态方法同步，全局锁)，这时如果其他线程来请求执行方法，会因为无法获得监视器锁而被阻断住。值得注意的是，如果在方法执行过程中，发生了异常，并且方法内部并没有处理该异常，那么在异常被抛到方法外面之前监视器锁会被自动释放​ 2、同步代码块：对于同步代码块，JVM采用monitorenter、monitorexit两个指令来实现同步。monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应，这样就保证了执行monitorexit指令的线程是monitor监视器的所有者。根据虚拟机规范的要求，在执行monitorenter指令时，首先要尝试获取栈顶对象的锁，如果这个对象没有被锁定，或者当前线程已经拥有了那个对象的锁，把锁的计数器加1，相应地，在执行monitorexit指令时会将栈顶对象锁计数器减1，当计数器为0时，锁就会被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到对象锁被另外一个线程释放为止。 对于同步方法或同步块，通过Class文件中的access_flags或monitorenter、monitorexit指令来标记执行这些代码时需要进行同步，但是具体如何进行同步呢？这就是接下来要分析的主要内容。再讲解同步之前，我们先来看下对象头，因为JVM中synchronized的实现关键就涉及到对象头的操作。 对象头Java中对象的内存布局主要分为三个区域：对象头(Header)、实例数据(Instance Data)和对齐填充(Padding)。synchronized的实现方式依赖于对象头，所以，这里我们先来简单介绍下Java中对象头。 实例数据区主要是实例属性数据存储区域，对齐填充在HotSpot中主要采用8字节对齐方式，对象头和实例数据区字节数不是8的倍数，采用对齐填充方式让其等于8的倍数。这里来看下对象头，如果是数组类型，其由MarkWord、length(数组长度)和Pointer，Pointer是指向该对象的元数据信息，即该对象的Class实例，对象的方法定义都是在Class实例中；如果是非数组类型，对象头只包含：MarkWord和Pointer两部分。 对象头具体描述如下：1、如果是数组类型，则使用3个字宽存储对象头，如果是对象非数组类型，则使用2个字宽，在32位JVM中，一个字宽等于4字节，而64位JVM中，一个字宽等于8字节，即在32-bit JVM上对象头占用8bytes，在64-bit JVM上对象头占用16bytes（开启指针压缩后占用4+8=12bytes）2、64位机器上，数组对象的对象头占用24 bytes，启用压缩之后占用16 bytes。之所以比普通对象占用内存多是因为需要额外的空间存储数组的长度，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中无法确定数组的大小。3、HotSpot虚拟机的对象头包括两部分信息：Mark Word(标记字段)和Klass Pointer(类型指针)4、Mark Word用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等5、对象需要存储的运行时数据很多，其实已经超出了32、64位Bitmap结构所能记录的限度，但是对象头信息是与对象自身定义的数据无关的额外存储成本，考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。例如在32位的HotSpot 虚拟机中对象未被锁定的状态下，Mark Word的32个Bits空间中的25Bits用于存储对象哈希码（HashCode），4Bits用于存储对象分代年龄，2Bits用于存储锁标志位，1Bit固定为0，在其他状态（轻量级锁定、重量级锁定、GC标记、可偏向）下对象的存储内容如下表所示： 上图展示的是32位机器下对象头的情况，64位情况下原理大致一样而且更简单，这里就不再介绍了。 MarkWord里默认数据是存储对象的hashcode等信息，但是会随着对象的运行改变而发生变化，不同的锁状态对应着不同的记录存储方式，当对象成为锁(被锁住)后，对象头里的MarkWord字段就会存储Monitor信息，Monitor信息可以理解为锁信息。 锁的状态可分为四种：无锁状态、偏向锁、轻量级锁和重量级锁，其实现原理要依赖对象头进行控制。再了解对象头的基础上，下面我们就可以分析每种锁的实现原理。 锁分类早期，synchronized属于重量级锁，效率低下，因为监视器锁(monitor)是依赖于底层的操作系统互斥Mutex Lock来实现的，而操作系统实现线程之间的阻塞、调度、唤醒等操作时需要从用户态切换到内核态，最后再由内核态切换到用户态，将CPU的控制权交由用户进程，用户态与内核态之间频繁的切换，严重影响锁的性能，这也是为什么早期的synchronized效率低的原因。 在Java 6之后Java官方对从JVM层面对synchronized进行较大优化，所以现在的synchronized锁效率也优化得很不错了，为了减少获得锁和释放锁所带来的性能消耗，引入了偏向锁、轻量级锁和自旋锁等概念，下面就来分析下它们的原理。 轻量级锁轻量级锁实现的背后基于这样一种场景假设：在真实生产环境下，我们程序中的大部分同步代码一般都处于无锁竞争状态，轻量级锁主要解决如下场景：线程A和线程B都要访问对象o的同步方法，但是它们之间不会同时访问，线程A访问完成后线程B再去访问，它们之间访问类似于交替访问，因此，这种情况下并不会产生锁竞争问题。在无锁竞争的情况下完全可以避免调用操作系统层面的重量级互斥锁，只需要依靠CAS原子指令就可以完成锁的获取及释放，但是当检测到存在锁竞争的情况下，轻量级锁就会膨胀为重量级锁。 下面通过如下同步代码块分析下轻量级锁实现的大致流程： 1234public class Obj &#123; public synchronized void fun1()&#123; &#125;&#125; ​ 1、当代码进入同步块时，即调用Obj.fun1()方法，当Obj实例为无锁状态，即对象头的锁标志位为01，当前线程会在栈帧中创建一个锁记录(Lock Record)，同时将锁对象Obj的对象头中MarkWord拷贝到锁记录中，因为栈是线程私有的，Java方法的调用就是通过栈帧得到入栈和出栈实现的，所以将锁记录保存到栈帧中，这一步主要完成MarkWord拷贝过程； ​ ​ 2、将MarkWord拷贝到Lock Record中完成后，尝试使用CAS将MarkWord更新为指向锁记录的指针，如果更新成功，当前线程就获得了锁，同时更新锁标志位为00，表示当前对象处于轻量级锁状态 ​ 3、更新失败情况主要如下：比如有两个线程A和线程B同时竞争锁，执行步骤1时由于当前对象处于无锁状态，所以这两个线程都会在它们的栈帧中创建Lock Record，然后将对象头中的MarkWord拷贝进去，然后它们都同时进入步骤2执行CAS原子操作将对象头中的锁指针指向自己栈帧中的Lock Record，所以，肯定有一个成功，另一个就会失败，成功的就是获取到偏向锁的线程，失败的就是没有获取偏向锁的线程。如果更新失败，JVM会先检查锁对象的MarkWord是否指向当前线程的锁记录，如果是则说明当前线程拥有锁对象的锁，可以直接进入同步块，这是重入锁特性，不是则说明其有其它线程抢占了锁 ​ 4、其它线程抢占了锁，说明存在锁竞争情况，这时轻量级锁并不为立即膨胀为重量级锁，而是进入自旋模式，自旋模式期间还是无法获取锁，就会膨胀为重量级锁，大致思路：尽量降低阻塞的可能性，这对那些执行时间很短的代码块来说有非常重要的性能提高。 为什么要进入自旋模式原因？ 膨胀为重量级锁会涉及到有用户态切换到内核态进行线程的休眠和唤醒操作，然后再切换到用户态，这些操作给系统的并发性能带来了很大的压力，共享数据的锁定状态可能只会持续很短的一段时间，为了这段时间去挂起和恢复线程并不值得，可以让后面请求锁的那个线程”稍等一下”，但不放弃处理器的执行时间，看看持有锁的线程是否很快就会释放锁。为了让线程等待，只需要让线程执行一个忙循环（自旋），所以自旋会对CPU造成资源浪费，特别是长时间无法获取锁的情况下，所以自旋次数一定要设置成一个合理的值，而不能无限自旋下去。JDK1.6默认是开启了自旋锁功能，而且对自旋次数也不在是固定值，而是通过一套优化机制进行自适应，简化了对自旋锁的使用。 注意：自旋在多处理器上才有意义，这理解也很简单：自旋是不会释放CPU资源的，在单处理器上如果某个线程处于自旋状态，也就意味着没有其它线程处于同时处于运行状态，也就在自旋期间不可能存在线程释放锁资源。所以，单处理上自旋是没有意义的，不过现在服务器一般不可能运行在单处理器上。 ​ 5、如何膨胀为重量级锁呢？​ a.步骤4中在自旋模式下依然无法获取锁，即会膨胀为重量锁​ b.首先当前线程会修改Obj对象头中锁标志位，由代表轻量级锁的00修改成代表重量级锁的10，然后当前线程进入休眠模式，当然了再进入休眠模式之前还会进行一些操作，这里先这么理解，后面分析重量锁时具体流程再分析说明​ c.当持有Obj对象偏向锁的线程执行完同步方法后，会通过一次CAS原子操作将对象头中的MarkWord由当前栈帧中的Lock Record进行重置回之前内容，如果重置成功，则释放锁完成；但是，根据上步骤我们知道，由于当前已膨胀为重量级锁，导致Obj对象的MarkWord中的锁标志位已被修改，CAS重置对象头操作会失败，这时就会感知到：在偏向锁运行期间，存在了其它线程竞争锁资源情况，当前锁已被膨胀为重量级锁，所以，在释放锁得到同时，会唤醒应等待该锁导致休眠的线程 轻量级锁是不支持”并发”，遇到”并发”就要膨胀为重量级锁。可能你会疑问：锁就是用来解决并发下资源同步问题，轻量级锁对“并发”都不支持要它能干什么呢？注意：此并发并非彼并发，这里的并发是带有引号的，即不存在锁竞争的并发。 轻量级锁在申请锁资源时通过一个CAS操作即可获取，释放锁资源时也是通过一个CAS操作即可完成，CAS是一种乐观锁的实现机制，其开销显然要比互斥开销小很多，这就是轻量级锁提升性能的核心所在。但是，轻量级锁只是对无锁竞争并发场景下的一个优化，如果锁竞争激烈，轻量级锁不但有互斥开销，还要多一次CAS开销，这是轻量级锁比重量级锁性能更差。所以，JVM检测到锁竞争时自动膨胀为重量级锁原因就在于此。 偏向锁轻量级锁优化了并发情况下串行化访问的场景，即下面示意图中的场景一，现在有个更极端情况：假如一段时间间隔内同步方法只会被同一个线程多次访问，即下面示意图中的场景二，从总体看同步方法是在单线程环境中运行。如果使用轻量级锁，每次调用同步方法要通过一次CAS操作申请锁，执行完后同样通过一次CAS操作释放锁，如下面场景二产生了7次调用共要执行14次CAS操作，还不包括其它开销。JVM工程师们对场景二进一步进行优化：只会在线程第一次调用同步方法时获取锁，执行完成后不去释放，后面该线程再次进入时不需要再次获取锁，直接进入，只有当其它线程申请锁时才会释放，因此，同样的场景二，偏向锁只会产生2次CAS操作。 偏向锁的引入，主要是JVM工程师们经过研究发现：在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁的代价而引入偏向锁。偏向锁是对轻量级锁的进一步优化，轻量级锁优化了并发时串行化执行的场景，而偏向锁是对并发时”单线程”场景的优化。 默认JVM是开启偏向锁特性，但是默认JVM启动后的的头4秒钟这个feature是被禁止的，这也意味着在此期间，prototype MarkWord会将它们的bias位设置为0，以禁止实例化的对象被偏向。4秒钟之后，所有的prototype MarkWord的bias位会被重设为1，如此新的对象就可以被偏向锁定了，当然也可以通过如下方式缩短这个延迟： 1-XX:+UseBiasedLocking -XX:BiasedLockingStartupDelay=0 偏向锁的MarkWord信息如下： 批量重偏向&amp;批量撤销 存在如下两种情况：​ 1、对于存在明显多线程竞争的场景下使用偏向锁是不合适的，例如生产者/消费者队列，生产者线程获得了偏向锁，消费者线程再去获得锁的时候，就涉及到这个偏向锁的撤销(revoke)操作，而这个撤销是比较昂贵的，而且在多生产者、多消费者情况下，这种状况更加糟糕，而且可能程序中使用了大量的这种队列，解决方案就是：识别出这些对象并禁止它们使用偏向锁特性；​ 2、还存在这样对象集，它们偏向的线程并不合适，但是重新偏向另外线程确实合适的，例如线程t1初始化了大量对象obj，然后对每个对象执行了用于初始化的同步方法，这样导致这组对象集偏向锁中的threadID都指向了t1，但是如果另外一个线程开始真正指向obj对象集上的同步方法，这就导致了大量偏向锁的revoke操作 怎么判断对象是否适合偏向锁呢？解决方案是：jvm采用以class类为单位的做法，其内部为每个类维护一个偏向锁计数器，对其对象进行偏向锁的撤销操作进行计数。当这个值达到指定阈值的时候，jvm就认为这个类的偏向锁有问题，需要进行重偏向（rebias），对所有属于这个类的对象进行重偏向的操作叫批量重偏向（bulk rebias）。 之前的做法是对heap进行遍历，但是这种实现方式如果堆增加到很大时是会存在性能问题的，后来便引入epoch。Class实例中包含了MarkWord原型–mark_prototype属性，该属性中的bias决定了该类型的对象是否允许被偏向锁定，与此同时，当前的epoch位也被保留在mark_prototype中。当需要bulk rebias时，对这个类的epcho值加1，以后分配这个类的对象的时候mark字段里就是这个epoch值了，同时还要对当前已经获得偏向锁的对象的epoch值加1，当然是在线程处于安全点时停止线程执行更新。对于那些正在运行且持有偏向锁的线程，由于没法更新导致对象头中的epoch和mark_prototype的epoch值不匹配，即偏向锁状态失效，下一个试图获取锁对象的线程使用原子CAS指令可将该锁对象绑定于当前线程。 偏向撤销(revoke)：如果一个新线程申请偏向锁，发现该对象已经处于偏向锁状态，就会去判断epoch是否有效且线程ID是否指向自己，如果无效或线程ID并没有指向自己，需要让偏向锁撤销并重新偏向自己。在重新偏向自己之前，还回去判断之前线程是否还在运行，如果还在运行是否还在继续使用锁，如果还在继续使用锁则产生锁竞争，偏向锁会被膨胀为轻量级锁，否则，新线程通过CAS原子操作将对象头中的线程ID重新偏向新线程。 批量重偏向导致对象头中的线程ID指向被重置为null，即线程重新通过CAS操作获取偏向锁。简单理解：批量重偏向是对当前类型下的对象偏向锁的一次校正，因为当前该类型的偏向锁存在大量的revoke被JVM判定是存在问题的偏向锁，批量重偏向后这个类的revoke计数器会被重置，如果这个类的revoke计数器继续增加到一个阈值，可能会继续进行一次批量重偏向，也可能不再继续批量重偏向，就这样继续1到多次批量重偏向后，jvm就认为这个类不适合偏向锁了，就要进行批量撤销(bulk revoke)，将该类的Class的mark_prototype中的bias属性设置成0，表示该类型下所有对象不允许被偏向锁定，同时将已存在的偏向锁膨胀为轻量级锁。 在批量重偏向(bulk rebias)的操作中，prototype的epoch位将会被更新；在批量吊销(bulk revoke)的操作中，prototype将会被置成不可偏向的状态——bias位被置0。 下面通过如下同步代码块分析下偏向锁实现的大致流程： 1234public class Obj &#123; public synchronized void fun1()&#123; &#125;&#125; ​ 1、检测对象类型class中的bias设置是否允许偏向锁特性，只有开启此特性才能使用偏向锁​ 2、检测Obj对象头中MarkWord锁标识位等于01，代表无锁状态或已处于偏向锁状态，否则不能进行偏向锁设置；​ 3、检测Obj对象头中MarkWord偏向标识位，如果等于0，表示当前对象处于无锁状态，通过一次CAS原子操作将对象头线程ID设置成当前线程ID，设置成功则获取偏向锁成功​ 4、检测对象头中MarkWord偏向标识位，如果等于1表示当前已处于偏向锁状态，然后检测MarkWord中的线程ID是否等于当前线程ID，不等于则进入步骤(5)，等于会再进行判断epoch是否等于Obj类型的Class实例中的epoch，不等于说明该偏向锁失效，进入步骤(5)，等于则表明获取偏向锁成功，进入同步方法​ 5、监测偏向锁指向的线程是否还在运行，没有运行则执行步骤(6)，否则继续判断该线程是否还在持有锁，如果没有持有则执行步骤(6)，如果线程还在持有锁，则说明产生了锁竞争，会在持有偏向锁线程运行到全局安全点（这个时间点上没有正在执行的代码）时挂起运行线程，并将偏向锁膨胀为轻量级锁​ 6、使用CAS原子操作将对象头MarkWord中的线程ID设置成当前线程ID 偏向锁的核心思想是，锁不存在多线程竞争，且一个线程获取锁后接下来继续获取该锁的概率更大，可见偏向锁模式下线程是不会主动去释放偏向锁，只有其它线程来竞争该偏向锁时才会考虑撤销或膨胀。偏向锁解决了一次CAS操作可以实现任意多次调用，节省了每次调用申请锁、释放锁性能消耗，避免了轻量级锁产生大量的CAS操作导致的性能消耗，从而提升锁性能。和轻量级锁一样，偏向锁并不能解决锁竞争问题，一旦遇到锁竞争偏向锁就会膨胀为轻量级锁，轻量级锁也不能解决锁竞争问题，为什么不直接膨胀为重量级锁呢？如果锁竞争不是很激烈或者竞争时间非常短暂，前面介绍过轻量级锁有个自旋模式，可以通过自旋模式补救避免因偶然的误差导致直接膨胀为重量级锁。如果自旋模式也无法解决，说明说竞争可能确实激烈，轻量级锁也无能为力了，只能膨胀为重量级锁。 另外，偏向锁也不适合像生产者/消费者这种线程交替获取锁模式，这样可能会导致产生大量的偏向锁撤销和重偏向操作，得不偿失。 重量级锁通过前面分析发现，无论是轻量锁还是偏向锁，都不能代替重量锁，都只是在无锁竞争或者竞争不是很激烈的情况下进行的一些性能优化，减少重量锁产生的性能消耗，并不能真正解决锁竞争问题。轻量锁和偏向锁都是重量锁的乐观并发优化，因为它们都是通过CAS原子操作尝试性获取锁，在锁竞争不是很激烈情况下，尝试性获取锁的概率当然就会很大，避免了由用户态切换到内核态，借助系统的Mutex Lock互斥锁实现线程协调的过程，但是一旦锁竞争激烈，还是需要借助于重量级锁特性才能解决。 synchronized的重量级锁是通过对象内部的一个叫做监视器锁（monitor）来实现的，监视器锁本质又是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的。而操作系统实现线程之间的切换需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么synchronized效率低的原因。 当锁被膨胀为重量级锁后，锁标识位会被设置成10，同时对象头会指向一个monitor对象，它会管理协调这些竞争锁资源的线程们。大致示意图如下： 流程如下： ​ 1、如果线程A执行Obj对象的同步方法，通过对象头查找到Monitor的位置，然后线程A会进入WaitQueue区域，该区域主要是用于存储所有竞争锁资源的线程，多个线程同时竞争锁资源，只会有一个线程竞争成功，其它线程就会存储到该区域中，该区域主要维护两个队列：​ a.Contention List：所有请求锁的线程将被首先放置到该竞争队列中​ b.Entry List：Contention List中那些有资格成为候选人的线程被移到Entry List，这个设计一方面也是从性能方面考虑：Contention List在高并发场景下不断的有新线程加入该队列，并且存在多个线程同时操作Content List，所以要进行同步控制，如果锁释放时直接从Contention List获取线程显然存在并发访问问题。所以，Owner线程首先会从Contention List迁移出一批线程到Entry List中，锁资源释放时从Entry List中获取线程，一般都是将Entry List的head赋值给OnCheck，Entry List不会存在并发访问问题，因为只有Owner线程才会从Entry List中提取数据，且也只有Owner才能从Contention List迁移线程到Entry List中，所以性能更好，只有等Entry List使用完为空时，Owner线程会再次从Contention List迁移一批线程放入到Entry List中 ​ 2、Ready Thread区域主要是存储下一个可以参与竞争锁资源的线程，等锁资源释放时让OnCheck指向的线程参与锁竞争，OnCheck一般指向的是Entry List的head位置。注意：等待队列中只会有一个线程参与竞争，一般是FIFO方式参与竞争，避免所有等待线程一起竞争锁资源造成性能问题。 OnCheck要竞争锁资源，而不是将Owner的锁资源直接传递给OnCheck线程，OnCheck只代表有资格竞争锁资源的线程，竞争锁资源就意味着可能会失败，失败就意味着这是一种非公平锁的实现机制。到底哪些线程会和OnCheck线程竞争锁资源呢？就是当前新加入申请锁资源的线程们，因为我们知道，只有申请锁资源失败的线程才会放入到Contention List，现在假如新加入的线程还在刚申请，走了狗屎运这时刚好Owner线程释放了锁资源，这就导致了这些新加入线程会和OnCheck一起竞争锁资源，这些新加入的线程可能优先竞争到锁资源，这就是非公平性的体现。这么做主要是从性能方面考虑，毕竟新线程如果竞争失败要做一大堆初始化工作然后放入到等待队列Contention List中，而OnCheck线程竞争失败只需要重新阻塞即可，显然工作量要小很多。但是，进入等待队列中的线程基本上是按照先进先出FIFO策略获取到锁资源的，因此，这种机制只会牺牲一定的公平性。另外，至少OnCheck线程还可以参与竞争，而不是从性能考虑直接让新线程获取到锁，避免等待队列中线程饿死现象。这里的实现和之前分析的ReentrantLock的思想基本一致，可以参考之前ReentrantLock实现机制加深对这块的理解。 ​ 3、Running Thread区域主要是存储当前获取到锁后正在运行的线程，使用Owner指向当前运行线程 ​ 4、Blocking Queue区域主要是存储那些获取到锁资源但是调用wait等方法被阻塞的线程，由于wait操作会释放当前锁，即Owner会被重置为null，当前线程进入WaitSet中，同时OnCheck线程参与锁竞争获取锁资源，等被阻塞的线程被唤醒后会被移入Entry List重新等待获取锁资源 只有获取到某个对象的锁时才能调用该对象的wait()让当前线程挂起，也就是如下代码： 1234567&gt; Object obj = new Object();&gt; public void fun2() throws InterruptedException &#123;&gt; synchronized (obj)&#123;&gt; obj.wait();&gt; &#125;&gt; &#125;&gt; &gt; 也就是只有获取obj对象的锁才能调用obj.wait()让当前线程挂起到obj对象上，同样唤醒该对象时也只有先获取obj锁时才能调用obj.notify()或obj.notifyAll()唤醒obj对象上阻塞的线程。这种设计是如何实现的呢？通过这里对Monitor结构的分析，你可能很容易就想到：​ 1、Monitor是线程私有的，也就是只会被当前锁资源持有线程就是Monitor对象的拥有者，即Owner指向的线程​ 2、只有Monitor的拥有者才能调用wait()方法释放监视锁，该线程进入阻塞队列，其它竞争锁资源新线程重新拥有Monitor线程​ 3、同理，只有Monitor拥有者才能调用notify()/notifyAll()，这时会从Blocking Queue队列中将阻塞线程移入到Entry List，等待重新获取锁​ 4、wait()、notify()和notifyAll()都是只有Monitor的拥有者线程才能调用，而Monitor的拥有者线程就是当前持有obj对象锁的线程 多个线程竞争锁资源借助底层系统的Mutex Lock互斥锁实现，需要由用户态切换到内核态，由内核协调哪个线程获取到锁，哪些线程无法获取到锁，获取锁失败的线程会被内核进行阻塞，线程阻塞才能释放CPU资源。系统执行完后，会由内核态重新切换到用户态，将CPU的控制权交给获取锁的线程进行执行。 内核切换属于操作系统范畴，想了解的可以自行搜索资料学习。这里大致简单描述下：​ 1、程序经过编译最终会被翻译成机器指令进行执行​ 2、如果程序执行的是“1+1”这种简单指令，CPU获取到这个指令后直接执行加操作即可，这时CPU处于用户态下，相当于用户进程调用CPU执行指令​ 3、但是如果程序执行的是读取外围设备IO、线程休眠、线程唤醒等操作，这种操作涉及到用户无法访问内存某些区域，出于安全考虑，用户进程需要将CPU的控制权交由内核，由内核代替用户进行执行这些操作，这就是用户态向内核态切换，内核代替用户执行完这些敏感指令后，然后再将CPU控制权重新交给用户进程，用户进程获得CPU控制权后继续执行后续指令，你可以简单认为：处于内核态时，CPU可以执行更多操作指令​ 4、你会发现，涉及到内核切换一般至少要切换两次，即：由用户态切换到内核态，将CPU控制权交给内核，内核执行完后，再由内核态切换到用户态，用户进程重新获取CPU控制权继续向下执行，内核切换还是比较耗费性能的，所以，synchronized底层优化才会出现偏向锁、轻量级锁等 JVM中通过对象监视器Monitor实现重量级锁，也大致了解了Monitor结构，对Monitor进一步抽象可以总结为：其核心就是两个队列，竞争锁队列和信号阻塞队列，前者用于线程互斥，后者用于线程协调。 上图非常形象生动的描述了Monitor本质，图中圆圈代表线程， 左边区域是竞争锁的线程排队区域，简称等待区，右边是曾经获取过锁由于wait()等操作导致线程挂起锁被剥夺排队区域，简称阻塞区，它们中的线程都拼命的争夺进入中心舞台的入场券，而且这张入场券只有一张，这就导致中心舞台只能同时容纳一个线程，当中心区域的线程任务执行完成后，退出时会把它持有的入场券交出来，此时，等待区和阻塞区中的线程又开始竞争，如此往复。 锁总结JVM底层实现synchronized同步时依赖的偏向锁、轻量级锁和重量级锁的大致原理也分析完成了，还记得轻量级锁中对象头指向Lock Record和重量级锁中对象头指向Monitor，可能你会比较好奇它们之间有什么区别吗？这里我试着总结下，主要区别如下：​ 1、Lock Record存储在线程栈的栈帧中，如果你了解栈帧应该知道，栈帧代表的是一个方法调用，当方法调用完成，该栈帧也会从栈中出栈，因此，如果线程执行完同步方法后释放锁时Lock Record也就不复存在了，这时的对象头会被恢复至之前的MarkWord内容，可以说Lock Record是线程独有的；​ 2、Monitor是线程私有，Monitor中Owner指向的线程就是Monitor的拥有者，注意这里的线程私有和上面Lock Record线程独有是有区别的：Lock Record随线程同步方法执行完成会被销毁，新线程获得锁后继续在自己的线程栈的栈中重新创建一个Lock Record，并让对象头指向自己即获得锁；而Monitor拥有者在进行锁释放时，是不会销毁Monitor对象的，而只会把Monitor中的Owner重置为null，表示当前没有线程持有锁，然后其它线程竞争锁资源，竞争成功的线程会被设置到Owner上，Monitor不会随着线程执行完同步方法而被销毁，这就表明Monitor不可能存储在线程栈中，而是存储在堆上；​ 3、Lock Record和Monitor在释放锁时的行为也存在很大差别：Lock Record释放锁时会被销毁，对象头会被重置为之前的MarkWord内容，然后有新线程申请锁时会重新创建Lock Record让对象头指向，而Monitor释放锁时，只会把Monitor中的Owner重置为null，也就是说Monitor释放锁时对象头是不会变化的​ 4、Monitor结构明显比Lock Record复杂，Lock Record主要保存对象头的MarkWord信息，由于结构太过简单导致Lock Record没法维护由于锁竞争导致的等待线程，最多也就是让它们自旋几下，并没有存储它们的区域，这就是轻量级锁无法解决锁竞争问题的本质。Monitor不但要保存对象头的MarkWord信息，还要使用队列维护等待线程和阻塞线程，因此，产生锁竞争时只能用重量级锁处理。另外，Lock Record结构简单才可以每次释放锁时销毁，申请锁时重新创建，而Monitor创建代价大的多，所以，一旦对象膨胀为重量级锁，初始化完Monitor后会被对象头一直指向该Monitor​ 5、由于重量级锁维护着复杂的Monitor结构，同时还要使用底层系统的Mutex Lock导致用户态/内核态之间的多次切换对性能的损耗，所以才出现偏向锁，轻量级锁优化在锁竞争不激烈时的性能，情不得已时才会启用重量级锁 锁是并发编程中非常重要的一个内容，解决了高并发场景下非原子操作导致的状态不一致问题，通过上篇博文 《并发编程锁之ReentrantLock总结》及这篇博文，已经对Java中锁的两种主要实现机制进行大致的分析，再去理解偏向锁、轻量级锁、重量级锁、自旋锁、重入锁、悲观锁、乐观锁等一堆曾经困扰我很久的锁概念时，可以非常清晰的、简明扼要的表达出它们之间的本质区别。 偏向锁、轻量级锁、重量级锁都是JVM底层实现synchronized同步时引入的概念，最开始synchronized采用的是重量级锁机制实现，采用复杂的Monitor锁+底层系统Mutex Lock，由于太过复杂的Monitor结构和频繁的用户态/内核态间的切换导致性能不足，JVM工程师们在JDK1.6版本中引入了偏向锁、轻量级锁对重量级锁进行优化。 偏向锁和轻量级锁都是解决无锁竞争场景下锁的性能问题，因为它们都无法维护由于锁竞争导致的线程等待问题，所以遇到锁竞争就懵逼了，还是只能用重量级锁来处理。首先来看下轻量级锁，主要是解决线程间交替访问同步块问题，由于是线程交替访问而不是同时访问，所以并不会产生锁竞争，就没有必要使用笨重的重量级锁；再来看下偏向锁，偏向锁就更极端了，偏向锁认为不但没有锁竞争，而且在一段时间t1内都是线程A访问同步块，另一段时间t2内都是另一个线程B访问同步块，这样t1时间段内线程A通过一次CAS获取锁后，即使访问完同步块也不用去释放锁，不管线程A调用同步块多少次，都只需要第一次调用时申请锁，后面通过简单的判断直接进入，用完后即可离开，不需考虑锁申请和释放的问题，直到时间t2线程B过来访问，这时会把锁重偏向到线程B即可。 偏向锁锁解决的是一个周期内“单线程”访问共享资源问题，连CAS操作都是能节省就尽量节省，轻量级锁解决的是一个周期内多线程交替访问共享资源问题，使用CAS操作消除底层系统的互斥，而重量级锁解决的是一个周期内同时访问共享资源问题，需要管理等待线程以及依赖于底层系统互斥指令。 再来说说自旋锁，自旋锁不是锁的种类，而是锁的一项特性，如同重入锁一样，自旋的目的无非是优化性能，比如轻量级锁膨胀为重量级锁及ReentrantLock在真正进入休眠之前都会进行自旋，因为一旦轻量级锁膨胀为重量级锁或ReentrantLock中的线程进入休眠状态，对锁的性能都会造成很大的影响，自旋是为了极力挽救避免锁进入更糟糕的情况。但是自旋也会带来一个问题，自旋状态下会一直占用CPU资源，如果长时间无法获取锁而一直自旋下去，对系统资源造成很大的浪费，但是到底自旋多久比较合适呢，这还真是一个比较难拿捏的问题，好在JDK已经引入了自适应自旋，JVM会根据它的监控统计信息进行优化，自动动态的计算出自旋的周期，而不再简单的一个固定值。另外，自旋锁在单核系统下是没有意义的，因为自旋线程占用了CPU希望其它线程尽快释放锁才好结束自旋，其它持有锁的线程由于无法获取到CPU资源所以在自旋期间不可能获取到锁，但是现在一般不可能是单核系统，所以，JDK已经默认开启了自旋特性。 重入锁也是锁的一项特性，而非种类，其实Java中的锁基本都是重入锁，不可重入性锁会导致自己锁死自己的问题，而且出现一旦锁死再也无法解锁的严重问题，重入锁就是线程获取锁期间可以继续获取该锁，主要是通过在锁中设置一个计数器count，用于统计重入次数，同理在释放锁时，只有释放同样次数情况下才可能完全释放锁。重入锁的代码大致如下： 12345public synchronized void fun1()&#123;//synchronized方法已保证进入方法中线程已经获取到当前对象的锁 synchronized (this)&#123;//这里再次获取当前对象锁，而且会成功，这就是重入锁特性 System.out.println("test"); &#125;&#125; 最后，再来看下乐观锁和悲观锁，这是从另一个维度对锁进行的分类，乐观锁、悲观锁与具体编程语言无关，基本所有的编程语言以及涉及到并发编程的系统中都会存在悲观锁和乐观锁，比如redis、oracle、elasticsearch等中都存在悲观锁和乐观锁的身影。乐观锁借助系统的原子性指令，对共享资源进行操作，即其在操作前并不会加锁控制同步块，而是乐观认为不会存在锁竞争所以没必要加锁，但是一旦操作失败就表示出现了锁竞争，乐观锁一般通过多次自旋方式进行多次尝试，直到操作成功，具体可以参看ReentrantLock源码中CAS+无限循环方式，这就是典型的乐观锁在Java中的实现。而悲观锁有如其名，悲观的认为操作一定会出现多线程竞争导致的同步问题，所以在对同步块操作之前，先锁起来，只有自己能操作共享资源，其它线程此时是无法访问共享资源的，这种控制多线程串行化访问共享资源方式虽然解决了线程安全问题，但是效率肯定是不高的。乐观锁在竞争不是太激烈的情况下，性能一般是高于悲观锁锁的，但是一旦在高并发下多线程竞争激烈，由于乐观锁失败的概率大大增加导致乐观锁不断尝试获取锁效率降低，性能反而可能会低于悲观锁，但在一般的生产中，大多数线程都是竞争不太激烈的情况，所以乐观锁的使用概率还是非常大的。 偏向锁和轻量级锁都是借助于CAS操作完成，可以理解为是乐观锁的一种实现，而重量级锁借助于底层系统互斥，可以看成是悲观锁的实现。 回过头来，对比synchronized和ReentrantLock实现机制，会发现它们在很多实现思想上如出一辙，虽然它们实现方式不一样，只有提炼出它们的设计思想才能掌握它们的核心本质，同时提升对并发编程的驾驭能力。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Lock</tag>
        <tag>synchronized</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程锁之ReentrantLock总结]]></title>
    <url>%2F2018%2F01%2F31%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E9%94%81%E4%B9%8BReentrantLock%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[锁之前我讲过，在并发编程中一个比较难解决的就是共享资源并发访问控制问题。如果同步做的不好，很容易出现不一致问题，从而导致业务逻辑的错误；但是如果对共享资源控制的过于严格，又很容易对性能造成很大的影响。在并发编程中，一方面多从大牛的源码中学习精巧的思想和结构设计；另一方面要对并发基础知识掌握的足够牢固，你才能游刃有余的结合些设计模式、架构思想做出些高质量的高并发、高性能的系统。并发编程对设计模式、架构设计是非常依赖的，因此，并发编程对经验的积累、知识的积累方面要求是比较高的。 刚才说过，共享资源在并发访问中很容易造成不一致问题，解决方案就是我们所熟知的悲观锁和乐观锁。悲观锁就如其名字一样：悲观锁认为并发访问一定会导致状态不一致问题，所以在并发操作前一定要锁住资源，让并发线程一个接一个串行化去访问。而乐观锁就不一样了，乐观锁认为并发访问在大多数情况下是不会导致状态不一致问题，所以可以放心的去访问，一旦出现问题再说，本质上乐观锁是不会对共享资源添加锁限制的。这是我个人的理解，可能直白不是那么的精准，有关悲观锁、乐观锁的定义可以搜索更准确的官方解释。 我们来看看在Java中是如何实现悲观锁和乐观锁的。悲观锁在Java中就是我们所熟知的锁，实现方式主要分为两种：synchronized和Lock，而乐观锁的实现主要通过CAS操作实现。这里我们来比较下synchronized和Lock方式的大致差别：​ 1、synchronized主要依赖JVM底层实现，而Lock是通过编码方式实现，其实现方式差别还是比较大​ 2、synchronized由于其简单方便，只需要声明在方法、代码块上即可，主要是不需要关心锁释放问题，在一般的编程中使用量还是比较大的，但是在真正的并发编程系统中，Lock方式明显优于synchronized：​ a.在高版本JDK中，已经对synchronized进行了优化，synchronized和Lock方式在性能方面差别已不太明显​ b.synchronized最致命的缺陷是：synchronized不支持中断和超时，也就是说通过synchronized一旦被阻塞住，如果一直无法获取到所资源就会一直被阻塞，即使中断也没用，这对并发系统的性能影响太大了；Lock支持中断和超时、还支持尝试机制获取锁，对synchronized进行了很好的扩展，所以从灵活性上Lock是明显优于synchronized的 在java.util.concurrent.locks包中有很多Lock的实现类，常用的有ReentrantLock、ReadWriteLock(实现类ReentrantReadWriteLock)，ReentrantLock在锁的使用上算是非常普遍的，这一节我们就以ReentrantLock为例，分析下Lock实现方式。 ReentrantLockReentrantLock是一个可重入的互斥锁，所谓可重入是线程可以重复获取已经持有的锁。锁基本上都是要支持可重入性，否则很容易出现死锁问题。比如：假如锁B不支持可重入性，线程A在持有锁B的情况下再次获取锁B，由于不支持可重入性导致线程A被阻塞，知道锁B资源被释放，但是锁B资源是被线程A持有的，所以线程A永远无法因获取到锁B而被唤醒，这就导致了死锁问题。 ReentrantLock内部实现主要通过AbstractQueuedSynchronizer类实现的，AbstractQueuedSynchronizer是抽象类，在ReentrantLock类中有两个实现类：NonfairSync和FairSync，分别对应非公平锁和公平锁的实现。类结构关系如下： 12345678910111213 +-------------------------------+ | AbstractQueuedSynchronizer | +--------------^----------------+ | | +------------------------------+ | Sync | +--------^----------^----------+ | | | |+-----------------------+ +-----------------------+| NonfairSync | | FairSync |+-----------------------+ +-----------------------+ ReentrantLock类内部持有一个Sync类型的变量，主要实现基本上都是调用Sync的实现机制，默认构建的是NonfairSync，即非公平锁，也可以通过带Boolean类型的构造函数构建公平锁，源码如下： 12345678910111213/*** 1、默认创建的非公平锁，性能更高，等价于ReentrantLock(false)*/public ReentrantLock() &#123; sync = new NonfairSync();&#125;/*** @param fair true:公平锁 false:非公平锁*/public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 获取锁原理使用ReentrantLock时，一般流程大致为：1、调用lock()申请锁资源，申请成功则立即返回，如果申请不到则会阻塞直到申请成功；2、申请锁成功后，即可进行共享资源操作；3、共享资源操作完成，最后调用unlock()释放锁资源。 123456789/** * 1、获取锁，如果该锁没有被其它线程持有则立即返回，并设置该lock的hold count=1 * 2、如果当前线程已经持有该锁，则lock的count+1并立即返回 * 3、如果该锁被其它线程持有，则当前线程处于休眠直到获取锁，获取锁同时设置hold count=1 * 4、sync.lock()会调用Sync具体实现类NonfairSync、FairSync中的lock()*/public void lock() &#123; sync.lock();&#125; ReentrantLock.lock()源码非常简单，调用sync.lock()，这就体现了ReentrantLock核心机制都是在Sync中实现的，上面已说过ReentrantLock中的Sync中有两个子类分别对应公平锁和非公平锁，这里我们就来先看非公平锁NonfairSync的实现： 123456789101112131415161718/*** state在AbstractQueuedSynchronizer类中定义，表示当前锁状态：** 0：当前锁锁未被任何线程持有，线程获取锁资源时可以使用CAS原子操作compareAndSetState(0, 1)* 将state由0修改成1，修改成功表示获取锁成功，state这时被修改成1了，并将exclusiveOwnerThread设置成当前Thread，* exclusiveOwnerThread即表示持有锁的线程** &gt;0：表示当前锁被线程持有，因为ReentrantLock是重入锁，同一个线程可以重入多次锁，每重入一次state加1，* 同样在释放锁资源release()的时候，每释放一次state减1，直到state=0表示全部释放完成，可以被其它线程竞争使用* state大于0时CAS原子操作compareAndSetState(0, 1)会失败，即进入另一分支*/final void lock() &#123; if (compareAndSetState(0, 1)) //当前线程获取锁成功，并将当前线程赋值给exclusiveOwnerThread变量 setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);//该分支则表示已有线程持有当前锁&#125; 这里的逻辑也很简单，通过一个CAS操作将state由0设置成1，成功则获取锁成功，并让Sync的exclusiveOwnerThread变量持有当前线程，供后续当前线程重入使用；如果CAS操作失败，则表示存在竞争，已有线程获取到锁，当前线程获取锁失败，需要进入acquire(1)分支。可以看到ReentrantLock的核心就是通过state字段的值判断是否被占用。 1234567891011121314151617/*** 1、调用tryAcquire尝试获取锁，注意：tryAcquire在AbstractQueuedSynchronizer类中实现直接抛出异常，一般是子类NonfairSync、FairSync继承重写该方法* 2、tryAcquire会做如下尝试：* a.如果state=0表示当前锁又没有被线程所持有，重新获取一次锁，成功返回true，失败返回false* b.如果持有锁的线程就是当前线程，则将state累加1，用于记录重入次数，释放的时候也要全部释放，并返回true表示获取锁成功* c.非以上两种情况，直接返回fasle* 3、如果获取锁成功，即tryAcquire返回true，则直接返回* 4、如果获取锁失败，即tryAcquire返回false，则将当前线程封装成Node放入到Sync Queue里(调用addWaiter)，等待Signal信号* 5、调用acquireQueued进行自旋的方式获取锁(有可能会 repeatedly blocking and unblocking)* 4、根据acquireQueued的返回值判断在获取lock的过程中是否被中断, 若被中断, 则自己再中断一下(selfInterrupt)*/public final void acquire(int arg) &#123; //acquireQueued的主要作用是把已经追加到队列的线程节点（addWaiter方法返回值）进行阻塞，但阻塞前又通过tryAccquire重试是否能获得锁，如果重试成功能则无需阻塞，直接返回 if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; AbstractQueuedSynchronizer中acquire()逻辑大致如下： ​ 1、首先调用tryAcquire()，该方法的目的主要是：a.重新自旋一次获取下锁看看是否成功，成功则返回true；b.判断持有锁的线程是否就是当前线程，如果是的话，直接在state累加1，并返回true表示获取锁成功；c.如果上述两个目的都没有实现，则返回false，表示获取锁失败。注意：tryAcquire()在AbstractQueuedSynchronizer中是直接抛出异常，具体调用的是子类NonfairSync中的实现逻辑，源码如下： 1234567891011121314151617181920212223/*** 1、该方法重新获取锁state，如果state=0表示当前又没有线程持有该锁了，则重新获取一次锁，成功返回true，失败返回false* 2、如果持有该锁的线程就是当前线程，则也会返回true表示获取锁成功，并将state累加acquires* 3、否则返回false表示获取锁失败*/final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123;//如果等于0表示此刻已没有线程持有该锁，所以重新获取一次锁，成功则立即返回true，否则立即返回false if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125;else if (current == getExclusiveOwnerThread()) &#123; //如果持有该锁的线程就是当前线程，则将state+1，然后立即返回true表示获取锁成功，这是可重入锁特性 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc);//修改state值，此处只会持有锁的线程才会执行，不存在多线程竞争情况，所以通过setState修改，而非CAS，这段代码实现了偏向锁的功能 return true; &#125; return false;&#125; ​ 2、如果tryAcquire()获取锁失败则执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg)语句，首先我们来看下addWaiter(Node.EXCLUSIVE)，它的作用就是将无法获取锁的线程追加到一个双向链表中，然后让线程休眠，当锁资源可用时会从该双向链表头部唤醒一个线程去竞争锁资源，其源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/*** 1、将无法获取锁的当前线程封装成Node加入到Sync Queue里面，其中参数mode是独占锁还是共享锁，null表示独占锁。如果是读写锁mode就为共享锁模式* 2、Sync Queue实现的一个双向链表，包含head和tail分别指向头部和尾部Node，head和tail设置为了volatile,这两个节点的修改将会被其他线程看到,主要是通过修改这两个节点来完成入队和出队* 3、当新加入Node时，将tail指向新加入的Node，同时之前的Node的next指向新Node，新Node的pre指向之前的tail节点，即在双向链表上添加节点成功* 4、总而言之，addWaiter的目的就是通过CAS把当前线程封装的Node追加到队尾，并返回该Node实例。* 5、把线程要包装为Node对象的主要原因：* a.构造双向链表时，需要指针前驱节点和后驱节点* b.Node中mode用于区分是否是排它模式还是共享模式* c.Node中的waitStatus用于表示当前线程状态： SIGNAL(-1) ：线程的后继线程正/已被阻塞，当该线程release或cancel时要重新唤醒这个后继线程 CANCELLED(1)：因为超时或中断，该线程已经被取消，其前驱节点释放锁后不会让处于该种状态的线程去竞争锁资源 CONDITION(-2)：表明该线程被处于条件队列，就是因为调用了Condition.await而被阻塞 PROPAGATE(-3)：传播共享锁 0：0代表无状态，默认Node就是该种状态，当存在后驱节点追加，就去把其前驱节点设置成SIGNAL*/private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); Node pred = tail; if (pred != null) &#123;//默认head = tail = null, tail !=null说明队列中已经有节点,直接CAS到尾节点 /** * 将当前Node追加到Queue尾部： * 1、将当前node的前驱节点设置成tail节点 * 2、通过cas操作将tail指针指向当前node * 3、并将之前的tail节点的next指向当前node * 通过上面三步骤，即将一个node追加到双向链表的尾部 */ node.prev = pred; if (compareAndSetTail(pred, node)) &#123;//4.CAS node到tail pred.next = node; return node; &#125; &#125; enq(node);//执行到这里，表明Queue队列为空,调用enq会初始化队列并将当前node追加到尾部 return node;&#125;/*** 1、这里通过一个死循环方式调用CAS，即使有高并发的场景，无限循环将会最终成功把当前线程追加到队尾* 2、第一次循环tail肯定为null，则会初始化一个默认的node，并将head=tail指向该node* 3、第二次循环的时候，会将当前node追加到1中创建的node尾部*/private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; //1.队列为空,初始化一个dummy节点,其实和ConcurrentLinkedQueue一样 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; ​ 3、addWaiter(Node.EXCLUSIVE)将当前线程封装的Node节点添加到等待锁资源的Queue上后，接下来要执行acquireQueued()，该方法是获取锁逻辑比较核心的一个方法，关键点有如下几个：​ a.该方法被设计成了一个无限for循环，只有满足通过tryAcquire()获取到锁时才会退出该循环，当然如果没有获取到锁也不会一直在for循环中进行空循环，而是通过parkAndCheckInterrupt()让线程休眠，当线程被唤醒后才会执行一次for循环看是否可以获取锁，获取成功则会将Queue的head指针指向当前thread，并将之前head废弃​ b.在tryAcquire()竞争锁资源时，会存在p == head判断，判断当前线程的前驱节点是否是head节点，只有前驱是head节点的线程才有资格调用tryAcquire()去竞争锁资源，这个设计思想逻辑主要是：申请锁资源失败的线程会依次加入到Queue中，head指向头部，tail指向尾部，如果head不为空，则该节点代表的线程为锁的占有者，当该线程释放锁时，它会唤醒它的后驱节点，而不是Queue中所有线程，因此，每次释放锁时只会唤醒一个线程，唤醒顺序也是从head到tail依次唤醒，而不是存在锁资源时一起唤醒然后竞争锁资源，因为这样如果存在几百几千个线程，同时竞争锁资源对系统性能损耗很大，有效的避免性能风暴​ c.该方法在让线程真正休眠前会让线程再次自旋一次获取锁，如果还是失败则立即进入休眠状态，作者这么设计就体现了：让线程休眠还是比较耗费性能资源的，涉及到上下文切换，另外当线程唤醒时可能会被分配到其它CPU上执行，由于高速缓存L1、L2是CPU独有的，就会降低高速缓存命中率，对性能影响还是比较大的，因此能尽量不休眠就不会让线程休眠​ d.当有线程释放锁时，会唤醒Queue头部线程的后驱节点，唤醒后依然要竞争锁，竞争的对象是刚申请锁资源还没有进入到Queue等待队列的线程们，如果竞争失败则再次进入休眠状态，这就体现了非公平锁的特性，这么设计的目的：主要从性能考虑，如果新申请锁的线程可以立即获取到锁，避免了后续一系列创建Node、添加Node到队列等一些列操作，而从Queue中唤醒的线程没有申请到锁只是重新进入休眠，代价要小很多，同时让它们一起竞争锁资源避免Queue等待队列中的线程一直无法获取锁而被饿死情况 123456789101112131415161718192021222324252627282930313233343536373839404142/*** 1、该方法是一个无限死循环，只有保证前驱节点就是头节点，并且重新调用一次tryAcquire()获取锁并成功，才会推送该循环* 2、否则会执行shouldParkAfterFailedAcquire()将当前node的前驱节点的waitStatus设置成SIGNAL，表示当它的前驱节点释放锁后会唤醒当前线程，然后当前线程就可以放心的让自己休眠了* 3、调用shouldParkAfterFailedAcquire()时，由于默认前驱节点的waitStatus不等于SIGNAL，所以会将前驱节点设置成SIGNAL，但是注意这时的返回结果是false，表示并不会立即让当前线程进入休眠状态，而是重新执行一次for循环，相当于给了一次重新获取锁的机会，如果获取锁成功，则将head节点指向当前节点，之前头结点就废弃了；如果获取失败则调用parkAndCheckInterrupt()让线程真正进入休眠状态4、parkAndCheckInterrupt()中调用LockSupport.park()让当前线程休眠，客户端也就进入阻塞状态，注意这里有个关键点：当休眠状态的线程被唤醒后，需要再次执行一次for循环通过tryAcquire()来竞争锁资源，竞争成功则退出当前for循环，当然也有可能会竞争失败，如果竞争失败会再次进去休眠状态** Queue队列中的线程是按照从头到尾部的顺序依次唤醒的，每次只会唤醒Queue中的一个线程，为什么还会出现竞争呢？这是因为虽然从Queue中只会唤醒一个线程，但是假如同时又有一个线程执行lock来获取锁资源，而此时并没有放入Queue等待队列中，它就会和从Queue中唤醒的线程进行竞争锁资源，这就体现了非公平锁的特性：后申请锁资源的线程可能会比先申请锁资源的线程优先申请到锁资源。* 为什么要这么设计呢？* 主要从性能考虑，如果新申请锁的线程可以立即获取到锁，避免了后续一系列创建Node、添加Node到队列等一些列操作，而从Queue中唤醒的线程没有申请到锁只是重新进入休眠，代价要小很多,同时让它们一起竞争锁资源避免Queue等待队列中的线程一直无法获取锁而被饿死情况*/final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; //1.获取当前节点的前驱节点 final Node p = node.predecessor(); //2.判断前驱节点是否是head节点(前继节点是head, 存在两种情况： // a.前继节点现在占用lock // b.前继节点是个空节点,已经释放lock,node现在有机会获取lock; 则再次调用tryAcquire尝试获取一下锁，该源码之前已经分析过 if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node);//3.获取lock成功,直接设置新head(原来的head可能就直接被GC回收) p.next = null; // help GC failed = false; return interrupted;//4.返回在整个获取的过程中是否被中断过；若整个过程中被中断过,则最后我在自我中断一下(selfInterrupt),因为外面的函数可能需要知道整个过程是否被中断过 &#125; //shouldParkAfterFailedAcquire(p, node)返回当前线程是否需要挂起，如果需要则调用parkAndCheckInterrupt()让当前线程休眠 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())//6.parkAndCheckInterrupt会把当前线程挂起，从而阻塞住线程的调用栈,返回值判断是否这次线程的唤醒是被中断唤醒 interrupted = true; &#125; &#125; finally &#123; if (failed)//7.在整个获取中出错 cancelAcquire(node);//8.清除 node 节点(清除的过程是先给 node 打上 CANCELLED标志, 然后再删除) &#125;&#125;private final boolean parkAndCheckInterrupt() &#123; //利用LockSupport的park方法来挂起当前线程的，直到被唤醒。 LockSupport.park(this); return Thread.interrupted();&#125; 看到这里，基本上对ReentrantLock非公平锁获取锁资源的流程有一个比较清晰的认识了。公平锁和非公平锁流程基本一致，区别只是在tryAcquire()获取锁逻辑的差别，具体如下： 12345678910111213141516171819FailSync类中的tryAcquire()获取锁逻辑：protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125;else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 可以看到，当锁资源可用(state=0)并且当Queue队列中不存在等待锁资源的线程时，才会通过cas操作将state由0设置成1，表示申请锁资源成功，否则都将加入到Queue队列的尾部。对比非公平锁获取锁资源逻辑nonfairTryAcquire()，差别主要是：非公平锁只要判断锁资源可用就会立即通过cas操作获取锁资源，而公平锁则会在锁资源可用的情况下，还要满足Queue队列中无等待锁资源线程才能立即申请锁资源，否则会被追加到Queue队列的尾部，这就体现了公平特性。 上面已经将ReentrantLock.lock()获取锁的流程基本都 分析完成，当然ReentrantLock还提供lockInterruptibly()、tryLock()、tryLock(long timeout, TimeUnit unit)等，如果对lock()逻辑比较清楚，这些方式获取锁的原理就比较简单了，下面大致说下。 ReentrantLock.lockInterruptibly()：可中断方式获取锁，通过之前源码分析，线程如果没有获取到锁，会通过LockSupport.park()方式休眠，当锁资源释放时，其它线程调用Lock.unpark()唤醒休眠线程去竞争锁资源，但是LockSupport.park()休眠的线程也可以通过中断方式进行唤醒，可中断锁就是在唤醒时候判断如是中断唤醒，则直接抛出异常，而lock()方式获取的锁使用中断唤醒后直接去竞争锁资源了，竞争不到直接休眠，这就是它们的差别，具体实现看源码： 123456789101112131415161718192021private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt())//进入阻塞状态，阻塞解除时，返回true表示中断方式唤醒 throw new InterruptedException();//中断唤醒时，不会去竞争锁资源，而是直接抛出异常 &#125; &#125; finally &#123; if (failed) cancelAcquire(node);&#125; ReentrantLock.tryLock()：可尝试性获取锁，获取到返回true，获取不到直接返回fasle，而不会阻塞，实现方式就更简单了，直接nonfairTryAcquire()获取锁，获取到立即返回true，获取不到立即返回false，而不是添加到Sync Queue阻塞队列中去等待。 ReentrantLock.tryLock(long timeout, TimeUnit unit)：会尝试一段时间，这段时间都无法获取锁就返回false 1234567891011121314151617181920212223242526272829303132private boolean doAcquireNanos(int arg, long nanosTimeout) throws InterruptedException &#123; if (nanosTimeout &lt;= 0L) return false; final long deadline = System.nanoTime() + nanosTimeout;//超时时间 //加入到Sync Queue，这里主要是性能优化，下面获取锁的逻辑在for无线循环中，如果超时时间设置较长 //一直无线循环下去肯定浪费CPU资源，所以会进行休眠，等待前驱节点释放锁时会唤醒该线程 final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return true; &#125; nanosTimeout = deadline - System.nanoTime(); if (nanosTimeout &lt;= 0L)//如果超时，直接返回false，表示获取锁失败 return false; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; nanosTimeout &gt; spinForTimeoutThreshold)//如果超时时间现在大于1000纳秒，就会进入休眠，否则就不停的for循环，因为超时时间太短，没必要进行休眠，比较休眠还是比较耗费资源的 LockSupport.parkNanos(this, nanosTimeout); if (Thread.interrupted())//可响应中断的，如果中断则会抛出异常 throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 总结通过上面对ReentrantLock源码分析，Lock机制的核心就是通过cas原子操作state属性，state=0表示锁资源可用，获取锁就是通过cas原子操作将state从0设置成1，成功就表示获取锁成功，如果state&gt;0,cas操作将会失败，即表示锁已被占用，当前获取锁失败。获取锁失败，根据是否是可中断、可超时等特性，处理的逻辑不太一致，但大致为：​ 1、将获取锁失败的线程封装成Node，封装成Node一方面是要构建双向队列，另一方面是Node中额外添加状态信息对节点进行控制​ 2、在一个for无线循环中通过Lock.park()让线程休眠，当有锁资源被释放发生时，会从队列头到尾的顺序依次唤醒线程(会跳过CANCELLED标记的节点，因为这些节点代表的线程已经无效了)，注意这里只会唤醒一个线程，唤醒的线程只表示该线程具有竞争锁资源的资格，还需要和新申请但还没有放入到Queue中的线程进行竞争该锁资源，这就是非公平锁的特性，这样设计主要是从性能方面考虑，如果竞争成功则退出for循环返回，否则继续进入休眠状态 最后再通过一个大致流程图，对整体的执行流程有个更清晰认识。 释放锁原理接下来我们来分析下ReentrantLock释放锁资源的流程。释放锁没有区分公平和非公平的，主要的工作就是减小state的值，当state等0的时候，释放锁并唤醒Queue中其他线程来获取锁。 123public void unlock() &#123; sync.release(1);&#125; release()是在AbstractQueuedSynchronizer中实现的， 123456789101112131415161718192021222324252627282930313233343536373839 /** * Releases in exclusive mode. Implemented by unblocking one or * more threads if &#123;@link #tryRelease&#125; returns true. * This method can be used to implement method &#123;@link Lock#unlock&#125;. * * @param arg the release argument. This value is conveyed to * &#123;@link #tryRelease&#125; but is otherwise uninterpreted and * can represent anything you like. * @return the value returned from &#123;@link #tryRelease&#125; */ public final boolean release(int arg) &#123; //tryRelease:尝试释放状态，返回true表示锁资源释放完成， if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false; &#125; /*** 1、只有持有该锁的线程才会执行tryRelease()，因此不会涉及到多线程问题，不需要使用cas保证原子性* 2、调用该方法会将state-1，然后判断state值，如果等于0表示当前线程已经释放锁资源完成，返回true，* 并将exclusiveOwnerThread设置成null，表示当前锁资源空闲，未被线程占用* 3、如果state&gt;0，则表示当前并未释放完全，返回false*/protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123;//只有state=0表示该锁被所有线程都释放完成，即锁可以被其它线程占用了，否则只是释放一次重入次数，并不会释放锁 free = true; setExclusiveOwnerThread(null); &#125; setState(c);//不存在并发问题，采用setState()而非cas操作，提供性能 return free;&#125; 深入分析上面通过源码已经对ReentrantLock获取锁和释放锁的大致流程有了比较清晰的认识，当你越深入分析时你会对Doug Lea这位大牛构思和多线程并发处理的游刃有余感到惊叹，以及后面我们会讲到的IO模型，依然会有Doug Lea大牛的精彩大作，如果你对他还不了解，可以多搜索关注下。 了解了流程并不一定就表示你已经完全熟悉ReentrantLock，你知道他是这么做的，但是你不一定清楚他这么做背后的考量是什么，毕竟并发编程比单线程编程复杂性高出太多，你很难顾及到所有线程分支运行的流程，这就是很容易导致bug的根源。上面我们已经分析过addWaiter()这个方法的作用，下面通过该方法进行更深入的分析，希望对并发编程的认识更加深刻。 addWaiter()方法主要完成工作：将未获取锁的线程封装成Node，然后追加到等待队列Queue尾部，等待队列Queue并不存在一个定义好的数据结构，而是通过head、tail、next和prev模拟出的具有出队、入队操作的双向链表。追加当前节点到双向链表尾部关键源码如下： 12345node.prev = pred;if (compareAndSetTail(pred, node)) &#123;//4.CAS node到tail pred.next = node; return node;&#125; 将上面代码梳理一下，大致分为三个步骤：​ 1、将当前node的prev指向tail节点；​ 2、通过cas原子操作将tail指针指向当前节点；​ 3、将之前tail节点的next指向当前节点。 示意图如下： 假如将追加节点的三个步骤顺序调换下，先将tail节点的next指向当前节点，然后cas原子修改tail指向，最后再来修改当前节点的prev指向，即将上面的1和3对调一下，会出现上面情况呢？ 将tail节点的next指向当前节点操作后，紧接着会执行cas操作修改tail指向当前节点，由于存在多线程并发问题，即可能会存在多个线程同时申请锁资源，假如现在t1、t2两个线程都同时做上面两个步骤：​ 1、t1线程修改next后，紧接着t2线程也修改next指向，导致会把t1修改的指向覆盖；​ 2、这时t1线程做cas替换tail指向成功后，t2也来做cas操作就会失败；​ 3、t1由于cas操作成功，最后修改prev指向 可以发现，由于并发导致追加的t1节点是存在问题的，正常情况下Node1的next应该指向t1节点，但是却被t2节点覆盖了。所以，1和3对调是在并发下是存在问题的。 假如1和2对调，先进行cas操作，然后修改prev，最后再来修改next又会怎么样呢？首先通过cas原子操作将tail指向当前节点，示意图如下： tail节点这时还是孤立的节点，prev和next都还没有指向，tail节点和其它节点之间没有关联了，这时如果其它线程需要遍历这个双向链表就比较危险了，比如释放锁时会调用unparkSuccessor()，其源码如下： 123456789101112131415161718private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); //获取当前节点的后继节点，如果满足状态，那么进行唤醒操作 // 如果没有满足状态，从尾部开始找寻符合要求的节点并将其唤醒 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123;//s.waitStatus&gt;0表示当前线程已被CANCELLED，不需要唤醒 s = null; //从tail向前查找，知道找到waitStatus&lt;=0的线程，赋值给s for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; //队列的维护（首节点的更换）是依靠消费者（获取时）来完成的，也就是说在满足了自旋退出的条件时的一刻，这个节点就会被设置成为首节点。 if (s != null) LockSupport.unpark(s.thread);&#125; 会发现其可能会存在一个从tail向前查找的流程，假如刚好这时执行这个流程，从tail向head查找节点显然就会存在问题，所以1和2对调的流程在并发下也是存在问题的。unparkSuccessor()在查找head的下一个有效节点的时候，没有从head到tail方向查找，而是反方向从tail向head查找，正常逻辑肯定是从head向tail方向查找速度更快，但是为啥反其道而行呢？如果你只看这段代码是永远看不出问题的，具体原因可以参加下面正常流程分析情况。 错误的顺序我就不一一举例了，大致都是差不多，现在我们来分析下为什么源码中这个顺序执行在并发下就不会存在问题。现在假设两个线程同一时间都没有获取到锁，都需要追加到Sync Queue队列尾部，大致流程如下：​ 1、线程t1的节点设置prev指向tail，线程t2节点同时也设置prev指向tail，这时就不会出现上面如果先设置next就会导致后设置把之前设置覆盖情况，因为如果先设置next是对Node1进行操作，存在多个线程对Node1同时操作导致状态不一致问题，而如果这里先设置prev，操作对象时线程本身的节点，是不存在多线程并发问题，示意图如下： ​ 2、这时t1和t2都进行cas原子操作，反正会有一个线程会操作成功，假如是t1线程操作成功，然后就可以顺利的设置Node1节点的next指向t1，因为只会存在一个线程操作成功，所以对Node1的操作此时也不会存在并发问题，由于t1的cas操作成功导致t2线程进行cas操作必然失败，此刻示意图如下： ​ 3、由于t2线程cas操作失败，因此不再继续操作Node1的next指向自己，而是进入enq()方法中，其源码如下，enq方法中通过cas+无限循环方式保证t2节点一定会被追加到Sync Queue尾部的，每次循环都是重新获取最新的tail，然后将t2的prev指向这个最新的tail，然后通过cas操作将tail指向自己，最后在将之前tail节点的next指向t2节点，这个案例中获取的最新tail就是t1节点了，所以t2节点会被追加到t1节点后，这样就能保证即使在高并发下依然可以实现节点正常添加，而不会像之前出现状态不一致情况，示意图如下： 1234567891011121314151617181920/*** 1、这里通过一个死循环方式调用CAS，即使有高并发的场景，无限循环将会最终成功把当前线程追加到队尾* 2、第一次循环tail肯定为null，则会初始化一个默认的node，并将head=tail指向该node* 3、第二次循环的时候，会将当前node追加到1中创建的node尾部*/private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; //1.队列为空,初始化一个dummy节点,其实和ConcurrentLinkedQueue一样 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; ​ 4、上面分析unparkSuccessor()在查找head的下一个有效节点的时候，没有从head到tail方向查找，而是反方向从tail向head查找，如果你对我刚才分析得到逻辑理解透彻的话，就比较好解释了。比如：t1设置prev指向Node1，然后cas操作将tail指向了t1，这时Queue的结构如下，假如这时候执行unparkSuccessor()，Node0查找它的后驱节点为Node1，假如Node1是无效节点，Node1需要继续查找它的后驱节点，但是这时Node1的next并没有设置，是无法查找到的，所以必须从tail向head方向查找才行。 通过对addWaiter的深入分析，你会对并发编程的难度有一个更加深刻的认识，真是处处要小心，搞不好就掉坑里面去了，但是Doug Lea巧妙的构思处理的游刃有余。 至此，对ReentrantLock也有了一个比较完整的流程分析，这一节也就结束了，后面会对Lock的其它实现类及synchronized的底层实现机制进行些分析。]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Lock</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算模型之Java8]]></title>
    <url>%2F2018%2F01%2F27%2F%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E4%B9%8BJava8%2F</url>
    <content type="text"><![CDATA[简述Java 8发行版是自Java 5（发行于2004，已经过了相当一段时间了）以来最具革命性的版本。Java 8我认为带来的最核心的变化主要体现在三个方面：Lambda(函数式编程)、Stream(流)和并发/并行编程简易化。其中Lamdba特性是最具革命性，为Stream、并行/并发编程等新特性提供基础支撑。 Java 8所做的改变，在许多方面比Java历史上任何一次改变都深远，为古老渐显疲态的Java注入新的活力：​ 1、lambda:最具革命性的新特性，直观上看：代码量大大减少，程序逻辑也清晰明了，可以编写出简单、干净、易读的代码；深层原因：函数式编程思想的风靡流行​ 2、Stream(流):Lambda和集合结合的产物，也让Lambda函数式编程这一酷炫技术得到很好展现的一个新特性， 是Java 8带给我们最核心、最实用的一个特性​ 3、并发/并行:借助于lambda、Stream和ForkJoin、CompletableFuture等新特性，让并发/并行编程更加简单 Lambda概述Lambda表达式并不是Java 8新出的概念，最近几年lambda表达式早已风靡于编程界，很多现代编程语言都把它作为函数式编程的基本组成部分，像Scala、Groovy等早已开始支持Lambda表达式语法。而且实践证明：让函数作为一等公民(Java是面向对象编程，只有对象是一等公民可以进行任意的传递，不过lambda底层实现依然采用的是对象封装实现的，因此，lambda可以看成是Java 8提供给程序员的一种”语法糖”)可以扩充程序员的工具库，从而让代码量大大减少、程序逻辑更清晰明了，编写出简单、干净、易读的代码，最终实现编程简单轻松和提高开发人员效率。 让Java实现一种尽可能轻量级的将代码封装为数据（Model code as data）的方法，这就是lambda表达式最本质的需求，用专家更精简的说法是：让行为参数化。 我们知道，Java作为一门面向对象的语言：在Java世界里，一切皆对象。在Java 8之前，Java专家们使用接口中的一个方法来封装对象行为进行参数传递，如可能存在如下情况： 12345button.addActionListener(new ActionListener) &#123; public void actionPerformed(ActionEvent e) &#123; ui.dazzle(e.getModifiers()); &#125;&#125; 使用接口方法定义行为，然后通过匿名类方式实现行为传递，但通常非常臃肿，既难于编写，也不易于维护。这种方案并不令人满意：冗余的语法会影响程序员在实践中使用行为参数化的积极性。像上面例子一样，真正有用的代码就只有一句：ui.dazzle(e.getModifiers())，而大量模板式的“噪点”代码充斥其中使代码复杂化，结构不够清晰。Java 8引入的lambda就是对这一问题很好的解决，通过向方法传递代码片段来解决这一问题，让你很简洁地对一个行为进行参数化并传递： 1button.addActionListener(e -&gt; ui.dazzle(e.getModifiers())) ###案例 123456789101112131415161718192021采用lambda可以轻松实现行为参数化，这样在接口设计的时候可以更抽象化、灵活性更高，如下： @Test public void test()&#123; System.out.println(operation((x,y)-&gt;x+y, 5));//实现累加求和 System.out.println(operation((x,y)-&gt;x*y, 5));//实现阶乘 System.out.println(operation((x,y)-&gt;x*x+y*y, 5));//实现其它需求 &#125; /** * 1、实现根据传入的n创建一个int类型序列：1,2,3,4...n * 2、将传入的行为函数去处理这个int类型序列 */ private int operation(IntBinaryOperator operator, int n)&#123; return IntStream.rangeClosed(1,n).reduce(operator).getAsInt(); &#125;通过使用lambda表达式，简单的几行代码就实现了一个灵活的数学运算方法，这就是lambda强大的地方。如果我们使用传统方式如何实现上述功能呢： 1、采用策略模式，首先定义一个接口，接口中定义一个方法代表数据处理的业务逻辑入口 2、执行运算的时候，通过匿名内部类将不同的业务逻辑封装到1步骤中定义的接口中，通过对象方式传入方法你可以自己用传统的策略模式实现上述功能，哪种方式编写简洁、灵活就一目了然了。 方法引用123456789101112131415161718192021222324若Lambda体中的内容有方法已经实现了，我们可以使用方法引用，可以理解为：方法引用是Lambda表达式的另外一种表现形式，对一类特殊的lambda表达式进一步进行简化。方法引用和下面的构造器引用都很简单，没什么实质性需要讲解的，本质上是对一类特殊的lambda进一步进行简化编写，可以看着是Java 8在极简编程方面的努力尝试。主要有三种语法格式： 1、对象::实例方法名 2、类::静态方法名 3、类:::实例方法名方法引用：lambda表达式参数类型和返回类型和引用方法的参数类型和返回类型一致//lamdba表达式方式Consumer&lt;String&gt; consumer = x -&gt; System.out.println(x);//方法引用方式consumer = System.out::print;//类::静态方法名Comparator&lt;Integer&gt; comparable = (x, y) -&gt; Integer.compare(x, y);comparable = Integer::compare;//类::实例方法名方式调用必须满足条件：两个参数，第一个参数是实例方法的调用者，而第二个参数是实例方法的参数时，即lamdba体中参数1.方法(参数2)，可以使用类::实例方法名//即：参数1作为调用方，参数2作为参数样式，也即，实例方法是参数1类型中的实例方法BiPredicate&lt;String,String&gt; bp = (x,y) -&gt; x.equals(y);bp = String::equals; 构造器引用1234567891011121314151617181920212223构造器引用，类似于方法引用:1、格式：ClassName::new2、与函数式接口相结合，自动与函数式接口中方法兼容。3、可以把构造器引用赋值给定义的方法，与构造器参数列表要与接口中抽象方法的参数列表一致class Employee&#123; public Employee(String str)&#123; System.out.println("str:"+str); &#125; public Employee()&#123; System.out.println("无参构造方法"); &#125;&#125;//构造器引用//构造器的参数列表要和Supplier中的T get()参数列表一致，所以调用的是无参构造方法Supplier&lt;Employee&gt; supplier = () -&gt; new Employee();supplier = Employee::new;//构造器的参数列表要和Function中的R apply(T t)参数列表一致，所以调用的是有参构造方法Function&lt;String,Employee&gt; fun1 = (x) -&gt; new Employee(x);Function&lt;String,Employee&gt; fun2 = Employee::new;fun2.apply("hah"); Stream概述lambda函数式编程完善了面向对象编程的不足，可以实现更抽象、更灵活的接口，从而让代码质量更加高效。对于lambda表达式我们理解到这点基本就足已，因为它还是比较简单的，真正核心的地方是lambda表达式与流式思想的结合。 Stream就是这样一个lambda表达式与流式思想的结合的产物，是lambda表达式作为流式思想实现的一个很好方案的展现。Stream的本质你可以这么理解：Stream就是将一系列lambda表达式进行串联形成一条数据处理流水线，实现通过各个lambda表达式间相互协作最终完成复杂的业务处理。Stream让传统的面向“存储” 的集合，具有了面向“计算”的能力。Java 8引入的Stream我认为是对开发人员来讲是实用性最好、使用率最高的一项特性，因为对数据集合的操作在日常开发中扮演着越来越重要的地位。 Stream意义传统的项目开发，基本上数据都会存储在关系型数据库如MySQL或Oracle中，利用关系型数据库提供的丰富内置函数，可以帮助完成各种数据处理工作，因此，传统的开发更多的是面向数据库编程。然后，现在是一个信息数据爆炸的时代，数据来源五花八门导致数据格式也不在是传统的以结构化数据为主的特点，而是非结构化、半结构化数据越来越多，这就对数据处理的需求更多、处理手段要更加灵活。而且数据规模越来越大，传统的关系型数据库已经无法满足需要，所以出现现在NoSQL遍地开花的局面。NoSQL数据库主要解决的核心关注点“海量数据存储和高效数据检索”，而和关系型数据库核心关注点“强一致性和强大的数据处理功能”是不一致的。这两方面的变化导致了现在越来越多的项目已将数据处理的需求移植到了程序开发中，数据处理和并发编程在日常开发中需求量大增。 传统的Java集合操作提供的API接口只有基本的功能性接口，比如：新增元素、删除元素、集合大小、集合iterator迭代，缺少面向实际业务层面更高层次的抽象接口，而Stream就是解决了传统集合操作的不足，使程序员得以站在更高的抽象层次上对集合进行操作。 Stream实现了类似SQL语法样式，使用声明式的类似描述性语言，有效帮助开发人员构建高质量、高性能的数据处理平台，挖掘出真正的商业价值，让“大数据”发挥出强大的威力。 案例12345678910111213141516171819202122232425262728293031323334案例一:下面的代码源自JDK中的Class类型（getEnclosingMethod方法），这段代码会遍历所有声明的方法，然后根据方法名称、返回类型以及参数的数量和类型进行匹配：for (Method method : enclosingInfo.getEnclosingClass().getDeclaredMethods()) &#123; if (method.getName().equals(enclosingInfo.getName())) &#123; Class&lt;?&gt;[] candidateParamClasses = method.getParameterTypes(); if (candidateParamClasses.length == parameterClasses.length) &#123; boolean matches = true; for (int i = 0; i &lt; candidateParamClasses.length; i += 1) &#123; if (!candidateParamClasses[i].equals(parameterClasses[i])) &#123; matches = false; break; &#125; &#125; if (matches) &#123; // finally, check return type if (method.getReturnType().equals(returnType)) &#123; return method; &#125; &#125; &#125; &#125;&#125;throw new InternalError("Enclosing method not found");通过使用流，我们不但可以消除上面代码里面所有的临时变量，还可以把控制逻辑交给类库处理。通过反射得到方法列表之后，我们利用 Arrays.stream将它转化为Stream，然后利用一系列过滤器去除类型不符、参数不符以及返回值不符的方法，然后通过调用findFirst 得到Optional&lt;Method&gt;，最后利用orElseThrow 返回目标值或者抛出异常。return Arrays.stream(enclosingInfo.getEnclosingClass().getDeclaredMethods()) .filter(m -&gt; Objects.equals(m.getName(), enclosingInfo.getName())) .filter(m -&gt; Arrays.equals(m.getParameterTypes(), parameterClasses)) .filter(m -&gt; Objects.equals(m.getReturnType(), returnType)) .findFirst() .orElseThrow(() -&gt; new InternalError("Enclosing method not found"));相对于未使用流的代码，这段代码更加紧凑，可读性更好，也不容易出错。 1234567891011121314151617181920212223242526272829303132333435363738394041案例二：基本集合操作案例(出至《Java8函数式编程》) Album：专辑，由若干曲目组成Track：曲目，专辑中的一支曲目现在要实现这样需求：现在要找出长度大于1分钟的曲目，并将曲目名字返回成一个Set集合传统方式：public Set&lt;String&gt; findLongTracks(List&lt;Album&gt; albums) &#123; Set&lt;String&gt; trackNames = new HashSet&lt;&gt;(); for(Album album : albums) &#123; for (Track track : album.getTrackList()) &#123; if (track.getLength() &gt; 60) &#123; String name = track.getName(); trackNames.add(name); &#125; &#125; &#125; return trackNames;&#125;使用Stream方式：public Set&lt;String&gt; findLongTracks(List&lt;Album&gt; albums) &#123; return albums.stream() .flatMap(album -&gt; album.getTracks()) .filter(track -&gt; track.getLength() &gt; 60) .map(track -&gt; track.getName()) .collect(toSet());&#125;对比总结： 1、代码量减少 2、结构更加清晰，一方面是收益于代码量的减少；另一方面，主要是使用类似描述性语言让代码的逻辑更加清晰。比如看到filter就晓得是进行过滤处理，看到map就晓得是一对一处理、看到collect就晓得是将流收集到集合中等等，这些接口本身就具有很强的描述性，简化了开发人员对代码的理解 flatMap、filter、map、collect等等，这些就是Stream提供的声明式的类似描述性语言的API接口，搞过Spark开发的人看着这些函数名称应该很熟悉了，和Spark编程提供的算子很类似，在语义上基本一致，对大数据开发人员具有很好的亲和力。仔细思考会发现它们在计算模型设计的思想有很多的相似性： 1、Spark中对数据集封装成RDD，然后使用一堆map、flatMap、filter等算子对RDD中的数据元素进行中间层处理，最后使用reduce、count、collect、foreach等聚合操作输出最终结果； 2、Java 8中使用Stream对数据集进行封装，使用map、flatMap、filter等lambda表达式对Stream中元素进行中间层处理，最后使用reduce、count、collect、foreach等lambda表达式进行结果的聚合输出。 3、而且中间层map、flatMap、filter等操作它们都具有惰性特性，只有在遇到需要对结果进行聚合输出时才会正在的执行。 当然了这里只是列举的Spark Core编程思想，Spark还提供了Spark SQL、Spark Stream、Spark MLlib等模块，提供了大数据“一站式”的基于分布式的解决方案，显然是Stream无法比拟的。但是parallel和sequential可轻易的让Stream在并行流和串行流之间进行转换，也提供了丰富的“算子”操作，因此，你也可以把Stream看着是“轻量级”、“单机版”的Spark编程框架实现。 进一步延伸 Storm中的Bolt、Spark中的算子和Java 8中的lambda，它们的思想和UNIX中的管道、责任链和pipeline模式中封装的处理单元是很类似的：对数据的处理封装成一个个处理单元，然后根据业务需要组装成一个线性或非线性链表或DAG(有向无环图)，让数据像水流一样沿着这个有向无环图经过处理单元进行层层过滤、处理。这是一种非常实用的经典计算模型或叫编程思想，有一句话很能体现这种思想的价值：“如果说Unix是计算机文明中最伟大的发明，那么，Unix下的Pipe管道就是跟随Unix所带来的另一个伟大的发明”。它体现的哲学思想：”Do one thing, Do it well”，程序应该只关注一个目标，并尽可能把它做好，让程序能够互相协同工作完成复杂任务。 Storm、Spark计算模型的强大以及Java 8中lamdba简洁性，让我们看到：虽然它们和传统的管道、责任链、Pipeline等模式在底层思想上如此的一致，但是在技术上还是向前迈出了很大一步，这就是技术积累进步的力量。 也再次印证了：火热的大数据时代让这一编程思想再次受到人们的关注，Java 8中及时引入lambda函数式编程，不在简简单单认为只是一种酷炫、装逼神器，而是一种迫切的真实需求体现，是顺应潮流的必然结果。 声明式编程一般通过编程实现一个系统，有两种思考方式。一种专注于如何实现，比如：“首先做这个，紧接着更新那个，然后……”举个例子，如果你希望通过计算找出列表中最昂贵的事务，通常需要执行一系列的命令：从列表中取出一个事务，将其与临时最昂贵事务进行比较；如果该事务开销更大，就将临时最昂贵的事务设置为该事务；接着从列表中取出下一个事务，并重复上述操作。这种“如何做”风格的编程非常适合经典的面向对象编程，有些时候我们也称之为“命令式”编程，因为它的特点是它的指令和计算机底层的词汇非常相近，比如赋值、条件分支以及循环，就像下面这段代码： 1234567891011Transaction mostExpensive = transactions.get(0);if(mostExpensive == null) throw new IllegalArgumentException("Empty list of transactions")for(Transaction t: transactions.subList(1, transactions.size()))&#123; if(t.getValue() &gt; mostExpensive.getValue())&#123; mostExpensive = t; &#125;&#125;另一种方式则更加关注要做什么。使用Stream API你可以指定下面这样的查询：Optional&lt;Transaction&gt; mostExpensive = transactions.stream().max(comparing(Transaction::getValue)); 这个查询把最终如何实现的细节留给了函数库。我们把这种思想称之为内部迭代。它的巨大优势在于你的查询语句现在读起来就像是问题陈述，由于采用了这种方式，我们马上就能理解它的功能，比理解一系列的命令要简洁得多。 采用这种“要做什么”风格的编程通常被称为声明式编程。你制定规则，给出了希望实现的目标，让系统来决定如何实现这个目标。它带来的好处非常明显，用这种方式编写的代码更加接近问题陈述了。 在声明式编程语言中最为我们所熟知的是SQL，这个被称为第三代半或第四代编程语言取得了巨大成功，很重要的一点：它可以使用声明式语言，而不必关注具体实现细节。这一特性让它的使用及其简单，也成为它广受欢迎的关键。 Stream遵循”做什么，而不是怎么去做”的原则，可以看成lambda函数式编程对声明式编程的具体实践：你只需要使用不相互影响的表达式，描述想要做什么，由系统来选择如何实现。你可以使用Stream将几个操作串接在一起，表达一个复杂的操作，这些都是函数式编程语言的特性。 并行流并行化操作流只需改变一个方法调用。如果已经有一个Stream对象，调用它的parallel方法就能让其拥有并行操作的能力。如果想从一个集合类创建一个流，调用parallelStream就能立即获得一个拥有并行能力的流，非常的简单就实现了并行化编程。并行流就是一个把内容分成多个数据块，并用不同的线程分别处理每个数据块的流。这样一来，你就可以自动把给定操作的工作负荷分配给多核处理器的所有内核，让他们都忙起来。 并行流内部使用了默认的ForkJoinPool，它默认的线程数量就是你的处理器数量，这个值是由Runtime.getRuntime().availableProcessors()得到的。但是你可以通过系统属性java.util.concurrent.ForkJoinPool.common.parallelism来改变线程池大小，如下所示：System.setProperty(“java.util.concurrent.ForkJoinPool.common.parallelism”,”12”);这是一个全局设置，因此它将影响代码中所有的并行流。反过来说，目前还无法专为某个并行流指定这个值。一般而言，让ForkJoinPool的大小等于处理器数量是个不错的默认值，除非你有很好的理由，否则我们强烈建议你不要修改它。 当然，也可以通过自定义ForkJoinPool并指定并行度方式实现局部并行度修改，方式如下： 1234567891011121314public void test() throws Exception &#123; ForkJoinPool pool = new ForkJoinPool(6);//指定并行度为6，下面并行流执行的时并行度即设置成6 Long ret = pool.submit(() -&gt; &#123; return LongStream.range(1, 100).boxed().collect(Collectors.toList()) .stream() .parallel() .map(x -&gt; &#123; System.out.println(Thread.currentThread().getName()+":"+x); return x * 2; &#125;) .reduce((x, y) -&gt; x + y) .get(); &#125;).get(); &#125; 这是一个多核的时代，并行流对提系统整体升性能是极具价值，最关键的是并行流和串行流间切换如此的简单，一个函数就可搞定，这是传统Java进行并行编程无法想象的。 Stream总结java 8的流式处理极大的简化了对于集合的操作，实际上不光是集合，包括数组、文件等，只要是可以转换成流，我们都可以借助流式处理，类似于我们写SQL语句一样对其进行操作。java 8通过内部迭代来实现对流的处理，一个流式处理可以分为三个部分：转换成流、中间操作、终端操作。它们之间的区别就是：中间操作输出还是流，一般是用于对流中元素进行处理，而终端操作一般是聚合操作，用于获取最终的结果。如下图： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134下面再通过一个案例再次体验下并行/并发编程方面Stream和传统编程巨大的区别蒙特卡洛模拟案例(出至《Java8函数式编程》)如果公平地掷两次骰子，然后将赢的一面上的点数相加，就会得到一个2~12的数字。点数的和至少是2，因为骰子六个面上最小的点数是1，而我们将骰子掷了两次；点数的和最大超不过12，因为骰子点数最多的一面也不过6点。我们想要得出点数落在2~12之间每个值的概率。 方式一：求出掷骰子的所有组合，比如，得到2点的方式是第一次掷得1点，第二次也掷得1点。总共有36种可能的组合，因此，掷得2点的概率就是1/36。方式二：使用1到6的随机数模拟掷骰子事件，然后用得到每个点数的次数除以总的投掷次数。这就是一个简单的蒙特卡洛模拟。模拟投掷骰子的次数越多，得到的结果越准确，因此，我们希望尽可能多地增加模拟次数(Spark案例中SparkPi，就是利用蒙特卡洛模拟求π值)。 /** * 模拟公平投掷两次筛子，赢面上点数和 * @return */ private int twoDiceThrows() &#123; ThreadLocalRandom random = ThreadLocalRandom.current(); int firstThrow = random.nextInt(1, 7); int secondThrow = random.nextInt(1, 7); return firstThrow + secondThrow; &#125; /** * 使用蒙特卡洛模拟法并行化模拟掷骰子事件 */ public Map&lt;Integer, Double&gt; parallelDiceRolls(int N) &#123; double fraction = 1.0 / N; return IntStream.range(0, N).parallel() .mapToObj(x -&gt; twoDiceThrows()) .collect(Collectors.groupingBy(side -&gt; side, Collectors.summingDouble(n -&gt; fraction))); //Collectors.summingDouble(n -&gt; fraction) 将n转成double类型fraction，并累加 &#125; public static void main(String[] args) &#123; Map&lt;Integer, Double&gt; ret = new Demo1().parallelDiceRolls(10000); ret.forEach((k,v) -&gt; System.out.println("点数:"+k+", 概率"+v)); &#125;输出结果： 点数:2, 概率0.027693 点数:3, 概率0.05557999999999999 点数:4, 概率0.083445 点数:5, 概率0.11120599999999999 点数:6, 概率0.139088 点数:7, 概率0.166841 点数:8, 概率0.138882 点数:9, 概率0.11072699999999999 点数:10, 概率0.083487 点数:11, 概率0.055407 点数:12, 概率0.027644使用传统方式实现上述同样效果代码：/** * @author 36410 * @Copyright © 2017 tiger Inc. All rights reserved. * @create 2017-12-01 17:11 * Description:通过手动使用线程模拟掷骰子事件 */public class ManualDiceRolls &#123; private static final int N = 100000000; private final double fraction; private final Map&lt;Integer, Double&gt; results; private final int numberOfThreads; private final ExecutorService executor; private final int workPerThread; public static void main(String[] args) &#123; ManualDiceRolls roles = new ManualDiceRolls(); roles.simulateDiceRoles(); &#125; public ManualDiceRolls() &#123; fraction = 1.0 / N; results = new ConcurrentHashMap&lt;&gt;(); numberOfThreads = Runtime.getRuntime().availableProcessors(); executor = Executors.newFixedThreadPool(numberOfThreads); workPerThread = N / numberOfThreads; &#125; public void simulateDiceRoles() &#123; List&lt;Future&lt;?&gt;&gt; futures = submitJobs(); awaitCompletion(futures); printResults(); &#125; private void printResults() &#123; results.entrySet() .forEach(System.out::println); &#125; private List&lt;Future&lt;?&gt;&gt; submitJobs() &#123; List&lt;Future&lt;?&gt;&gt; futures = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; numberOfThreads; i++) &#123; futures.add(executor.submit(makeJob())); &#125; return futures; &#125; private Runnable makeJob() &#123; return () -&gt; &#123; ThreadLocalRandom random = ThreadLocalRandom.current(); for (int i = 0; i &lt; workPerThread; i++) &#123; int entry = twoDiceThrows(random); accumulateResult(entry); &#125; &#125;; &#125; private void accumulateResult(int entry) &#123; results.compute(entry, (key, previous) -&gt; previous == null ? fraction : previous + fraction ); &#125; private int twoDiceThrows(ThreadLocalRandom random) &#123; int firstThrow = random.nextInt(1, 7); int secondThrow = random.nextInt(1, 7); return firstThrow + secondThrow; &#125; private void awaitCompletion(List&lt;Future&lt;?&gt;&gt; futures) &#123; futures.forEach((future) -&gt; &#123; try &#123; future.get(); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;); executor.shutdown(); &#125;&#125;对比： 1、使用Stream几行代码就可以搞定一个复杂的并发编程，而使用传统方式却需要几十行到上百行等，代码的简洁性体现的淋漓尽致 2、得益于Stream编程使代码量大大减少，同时采用的是声明式编程，采用Stream编程方式结构流程清晰明了，而使用传统方式构建的程序流程就不是那么容易理解了 3、对集合执行操作流水线，并可以通过简单的一个方法就实现了串行流与并行流间的切换，基本上不会付出任何代价，传统方式在并发编程方面，又是创建管理线程池、又是要创建管理线程、最后通过异步方式获取结果等，显然代价要昂贵的多，这也是体现出传统编程下对使用并发编程积极性不高的一个因数，然而在当代这个多核时代，为了尽可能发挥出多核硬件资源的优势，并发编程是不可避免的，而且重要性也会越来越高 Stream性能分析Stream接口可以让你不用太费力气就能对数据集执行并行操作。它允许你声明性地将顺序流变为并行流。可以通过对收集源调用parallelStream方法来把集合转换为并行流。并行流就是一个把内容分成多个数据块，并用不同的线程分别处理每个数据块的流。这样一来，你就可以自动把给定操作的工作负荷分配给多核处理器的所有内核，让它们都忙起来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121案例：累加求和方式一：传统方式 public long iterativeSum(long n) &#123; long result = 0; for (long i = 1L; i &lt;= n; i++) &#123; result += i; &#125; return result; &#125;方式二：串行流方式 public long sequentialSum(long n) &#123; return Stream.iterate(1L, i -&gt; i + 1) .limit(n) .reduce(0L, Long::sum); &#125;方式三：并行流方式 public long parallelSum(long n) &#123; return Stream.iterate(1L, i -&gt; i + 1) .limit(n) .parallel() .reduce(0L, Long::sum); &#125;使用parallel()可以把流转换成并行流，从而让前面的函数归约过程（也就是求和）并行运行。Stream在内部分成了几块。因此可以对不同的块独立并行进行归纳操作，最后，同一个归纳操作会将各个子流的部分归纳结果合并起来，得到整个原始流的归纳结果。 配置并行流使用的线程池看看流的parallel方法，你可能会想，并行流用的线程是从哪儿来的？有多少个？怎么自定义这个过程呢？并行流内部使用了默认的ForkJoinPool，它默认的线程数量就是你的处理器数量， 这个值是由Runtime.getRuntime().availableProcessors()得到的。但是你可以通过系统属性java.util.concurrent.ForkJoinPool.common.parallelism来改变线程池大小，如下所示：System.setProperty("java.util.concurrent.ForkJoinPool.common.parallelism","12");这是一个全局设置，因此它将影响代码中所有的并行流。反过来说，目前还无法专为某个并行流指定这个值。一般而言，让ForkJoinPool的大小等于处理器数量是个不错的默认值，除非你有很好的理由，否则我们强烈建议你不要修改它。 我们声称并行求和方法应该比顺序和迭代方法性能好。然而在软件工程上，靠猜绝对不是什么好办法！特别是在优化性能时，你应该始终遵循三个黄金规则：测量，测量，再测量。为此，你可以开发一个方法，如下所示(调用10次，取耗时最短一次)： public long measureSumPerf(Function&lt;Long, Long&gt; adder, long n) &#123; long fastest = Long.MAX_VALUE; for (int i = 0; i &lt; 10; i++) &#123; long start = System.nanoTime(); long sum = adder.apply(n); long duration = (System.nanoTime() - start) / 1_000_000; System.out.println("Result: " + sum); if (duration &lt; fastest) fastest = duration; &#125; return fastest; &#125;测试一：for循环原生类型： public long iterativeSum1(long n) &#123; long result = 0; for (long i = 1L; i &lt;= n; i++) &#123; result += i; &#125; return result; &#125;输出内容：for -&gt; 原生类型耗时(毫秒):5 测试二：for循环装箱类型：public long iterativeSum2(Long n) &#123; Long result = 0L; for (Long i = 1L; i &lt;= n; i++) &#123; result += i; &#125; return result; &#125;输出内容：for -&gt; 装箱类型耗时(毫秒):126 测试三：iterator串行 public long sequentialSum(long n) &#123; return Stream.iterate(1L, i -&gt; i + 1) .limit(n) .reduce(0L, Long::sum); &#125;输出内容：iterator -&gt; 串行(毫秒):118测试四：iterator并行 public long parallelSum(long n) &#123; return Stream.iterate(1L, i -&gt; i + 1) .limit(n) .parallel() .reduce(0L, Long::sum); &#125;输出内容：iterator -&gt; 并行(毫秒):308测试五：LongStream.rangeClosed串行 public static long rangedSum(long n) &#123; return LongStream.rangeClosed(1, n) .reduce(0L, Long::sum); &#125;输出内容：LongStream.rangeClosed -&gt; 串行(毫秒):4 测试六：LongStream.rangeClosed并行 public static long parallelRangedSum(long n) &#123; return LongStream.rangeClosed(1, n) .parallel() .reduce(0L, Long::sum); &#125;输出内容：LongStream.rangeClosed -&gt; 并行(毫秒):1 结果分析： 1、测试一和测试二对比：5ms VS 126 ms 分析：a.同是for循环累加求和操作，原生类型和装箱类型耗时相差几十倍，因为装箱的对象必须拆箱成数字才能求和，存在性能消耗 b.原生类型long占用8字节，而它的装箱类型是一个对象，对象在内存中存储的布局可以分为三块区域：对象头、实例数据和对齐填充，以64位JVM为例，装箱Long占用16+8=24字节，刚好是对齐不需要进行填充，可见Long装箱类型是原生类型long在内存占用上相差3倍，而Integer装箱类型和int原生类型相差6倍 c.从上面两点分析可以得出：在进行大量数据处理的时候，可以使用原生类型尽量使用原生类型，而不要使用装箱类型，不论在性能还是内存占用上，原生类型都极具优势 2、测试三和测试四对比：118ms VS 308ms 分析：看到这个结果，可能感到很惊讶，为什么并行流比串行流性能低那么多，按道理并行流应该比串行流快很多才符合逻辑。 原因：采用Stream.iterate()生成的流是没法分成多个独立块来并行执行，因为每次应用这个函数都要依赖前一次应用的结果，导致数据集合在归纳过程开始时是没有准备好的，因而无法有效地把流划分为小块来并行处理。但是把流标记成并行，你其实是给顺序处理增加了开销，它还要把每次求和操作分到一个不同的线程上。这就说明了并行编程可能很复杂，有时候甚至有点违反直觉。如果用得不对（比如采用了一个不易并行化的操作，如iterate），它甚至可能让程序的整体性能更差，所以在调用那个看似神奇的parallel操作时，了解背后到底发生了什么是很有必要的 3、测试一和测试四对比：5ms VS 118ms 分析：为什么采用Stream方式累加求和竟比传统for循环方式性能低那么多，相差几十倍 原因：采用Stream.iterate()生成流中元素是装箱类型，计算时要进行拆箱后才会进行累加求和计算，导致了性能差。从另一点也可以印证这个观点：测试2采用for循环装箱类型耗时126ms基本接近Stream.iterate()串行流性能，可见性能的差距主要体现在拆箱上面 4、测试五和测试三、测试一对比：4ms VS 118ms VS 5ms 分析：测试五性能要比测试三性能高很多 原因：a.Java8意识到原生类型和装箱类型性能差距的问题，因此提供了一些列直接生成原生类型的流的类，如LongStream、IntegerStream等，通过LongStream.rangeClosed()直接产生原生类型long，而不再是装箱类型Long b.测试五和测试一性能基本接近，这也说明采用Stream这套接口开发在给我们带来方便的同时，性能是没有打折的 5、测试六和测试五对比：1ms VS 4ms 分析：测试六采用并行流，并行流使用系统内置的ForkJoinPool线程池，线程数默认是Runtime.getRuntime().availableProcessors()，我当前硬件平台是4核，而这里任务是CPU密集型而非IO密集型，所以相差4倍符合预期。 并发/并行编程由于摩尔定律在处理器的时钟频率不断提升这一方式遇到了瓶颈，即单核CPU在性能上无法进一步获得有效提升，现在趋势是在横向上进行扩展，即无法获取更快的CPU核心，但是可以通过获取更多的CPU核心来提升性能，这就是所谓的多核时代的来临。由单核主频的提升到多核扩展这一硬件结构的转变，为了让你的代码运行得更快，需要你的代码具备并行运算的能力，可以让每个处理线程单独占据一个核，从而得到多倍的整体性能。 前面提到的并行流就是Java 8对并行编程一个很好的实践，但流类库提供的数据并行化只是其中的一种形式，下面会介绍Java 8在中并发/并行编程的其它手段。 并发/并行编程区别并发是两个任务共享时间段，并行则是两个任务在同一时间发生，比如运行在多核CPU上。如果一个程序要运行两个任务，并且只有一个CPU 给它们分配了不同的时间片，那么这就是并发，而不是并行。两者之间的区别如下图： CompletableFutureFuture接口是在Java5中被引入，设计初衷是对将来某个时刻会发生的结果进行建模，它建模了一种异步计算，返回一个执行运算结果的引用，当运算结束后，这个引用被返回给调用方。打个比方，你可以把它想象成这样的场景：你拿了一袋子衣服到你中意的干洗店去洗。干洗店的员工会给你张发票，告诉你什么时候你的衣服会洗好（这就是一个Future事件）。衣服干洗的同时，你可以去做其他的事情。Future的另一个优点是它比更底层的Thread更易用。要使用Future，通常你只需要将耗时的操作封装在一个Callable对象中，再将它提交给ExecutorService，就万事大吉了。下面这段代码展示了Java 8之前使用Future的一个例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849ExecutorService executor = Executors.newCachedThreadPool();Future&lt;Double&gt; future = executor.submit(new Callable&lt;Double&gt;() &#123; public Double call() &#123; return doSomeLongComputation(); &#125;&#125;);doSomethingElse();try &#123; Double result = future.get(1, TimeUnit.SECONDS);&#125; catch (ExecutionException ee) &#123; // 计算抛出一个异常&#125; catch (InterruptedException ie) &#123; // 当前线程在等待过程中被中断&#125; catch (TimeoutException te) &#123; // 在Future对象完成之前超过已过期&#125;我们很难表述Future结果之间的依赖性；从文字描述上这很简单，“当长时间计算任务完成时，请将该计算的结果通知到另一个长时间运行的计算任务，这两个计算任务都完成后，将计算的结果与另一个查询操作结果合并”。但是，使用Future中提供的方法完成这样的操作又是另外一回事。这也是我们需要更具描述能力的特性的原因，比如下面这些。 1、将两个异步计算合并为一个——这两个异步计算之间相互独立，同时第二个又依赖于第一个的结果 2、等待Future集合中的所有任务都完成 3、仅等待Future集合中最快结束的任务完成（有可能因为它们试图通过不同的方式计算同一个值），并返回它的结果 4、通过编程方式完成一个Future任务的执行（即以手工设定异步操作结果的方式） 5、应对Future的完成事件（即当Future的完成事件发生时会收到通知，并能使用Future计算的结果进行下一步的操作，不只是简单地阻塞等待操作的结果）。CompletableFuture类（它实现了Future接口）如何利用Java 8的新特性以更直观的方式将上述需求都变为可能。Stream和CompletableFuture的设计都遵循了类似的模式：它们都使用了Lambda表达式以及流水线的思想。从这个角度，你可以说CompletableFuture和Future的关系就跟Stream和Collection的关系一样。使用CompletableFuture很容易就构造出一个异步方法，而Java 8之前只能面向接口Callable构造，如果存在多个异步方法，就需要定义构造多个Callable接口的实现类，显然要繁琐很多： public Future&lt;Double&gt; getPriceAsync(String product) &#123; CompletableFuture&lt;Double&gt; futurePrice = new CompletableFuture&lt;&gt;(); new Thread( () -&gt; &#123; try &#123; double price = calculatePrice(product); futurePrice.complete(price);//如果价格计算正常结束，完成Future操作并设置商品价格 &#125; catch (Exception ex) &#123; futurePrice.completeExceptionally(ex);//否则就抛出导致失败的异常，完成这次Future操作 &#125; &#125;).start(); return futurePrice; &#125;我们已经了解了如何通过CompletableFuture编程方式构建异步方法，看起来这些操作也比较方便，但还有进一步提升的空间，CompletableFuture类自身提供了大量精巧的工厂方法，使用这些方法能更容易地完成整个流程，还不用担心实现的细节。比如，可以将上面的代码进一步简化： public Future&lt;Double&gt; getPriceAsync(String product) &#123; return CompletableFuture.supplyAsync(() -&gt; calculatePrice(product)); &#125;从这里再次见识到：Java 8在代码简洁性上做的比之前优秀太多了，lambda让行为参数化后，基本上模板化的代码都被封装起来了，开发人员只需关注真正与业务相关的核心代码片段即可，在提高代码开发效率的同时，代码质量也得到了很大提升。这就得益于声明式编程，你只需要关注你要做什么，而无需关注怎么做，具体实现的细节都被封装到框架中了，自然你的代码质量也就提升了。 Stream与CompletableFuture结合案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758案例：最佳价格查询器(案例来源《Java8 实战》) 需求：现在有个商家列表，你需要实现一个方法，它接受产品名作为参数，返回一个字符串列表，这个字符串列表中包括商店的名称、该商店中指定商品的价格：商家列表：List&lt;Shop&gt; shops = Arrays.asList(new Shop("BestPrice"), new Shop("LetsSaveBig"), new Shop("MyFavoriteShop"), new Shop("BuyItAll"));方式一：采用顺序查询所有商店的方式实现的findPrices方法public List&lt;String&gt; findPrices(String product) &#123; return shops.stream() .map(shop -&gt; String.format("%s price is %.2f", shop.getName(), shop.getPrice(product))) .collect(toList());&#125;方式二：对findPrices进行并行操作public List&lt;String&gt; findPrices(String product) &#123; return shops.parallelStream() .map(shop -&gt; String.format("%s price is %.2f", shop.getName(), shop.getPrice(product))) .collect(toList());&#125;假如商家查询价格是个比较耗时的接口，可能也是最快的改善方法是使用并行流来避免顺序计算。但是注意：Stream并行流使用的是内置的ForkJoinPool连接池中的执行线程（Executor）运行，直线线程默认等于CPU核数，如果商家列表比较多且查询价格接口存在IO阻塞情况，使用并行流并不能完全利用好CPU资源，当然也可以修改线程数使线程数加大，但是这个修改是全局性的，所以一般是不建议修改的，毕竟全局性的修改可能会导致对其它地方的使用造成影响。这就是为什么Stream叫做并行流而不是并发流。 方式三：使用CompletableFuture发起异步请求//自定义线程池执行器private final Executor executor = Executors.newFixedThreadPool(Math.min(shops.size(), 100), new ThreadFactory() &#123; public Thread newThread(Runnable r) &#123; Thread t = new Thread(r); t.setDaemon(true); return t; &#125; &#125;); public List&lt;String&gt; findPrices(String product) &#123; List&lt;CompletableFuture&lt;String&gt;&gt; priceFutures = shops.stream() .map(shop -&gt; CompletableFuture.supplyAsync( () -&gt; shop.getName() + " price is " + shop.getPrice(product)), executor)//这里需要指定使用的线程池，如果不指定，默认也是使用ForkJoinPool线程池 .collect(Collectors.toList()); return priceFutures.stream() .map(CompletableFuture::join) .collect(toList()); &#125;&#125;并行——使用流还是CompletableFutures？目前为止，你已经知道对集合进行并行计算有两种方式： 1、要么将其转化为并行流，利用map这样的操作开展工作 2、要么枚举出集合中的每一个元素，创建新的线程，在CompletableFuture内对其进行操作。后者提供了更多的灵活性，你可以调整线程池的大小，而这能帮助你确保整体的计算不会因为线程都在等待I/O而发生阻塞。 我们对使用这些API的建议如下。 1、如果你进行的是计算密集型的操作，并且没有I/O，那么推荐使用Stream接口，因为实现简单，同时效率也可能是最高的（如果所有的线程都是计算密集型的，那就没有必要创建比处理器核数更多的线程）。 2、反之，如果你并行的工作单元还涉及等待I/O的操作（包括网络连接等待），那么使用CompletableFuture灵活性更好，你可以像前文讨论的那样，依据等待计算，或者W/C的比率设定需要使用的线程数。 3、CompletableFuture具有一定的优势，因为它允许你对执行器（Executor）进行配置，尤其是线程池的大小，让它以更适合应用需求的方式进行配置，满足程序的要求，而这是并行流API无法提供的。让我们看看你怎样利用这种配置上的灵活性带来实际应用程序性能上的提升。 CompletableFuture实现多个异步任务流水线式操作12345678910111213141516171819202122232425262728上面介绍的主要是通过CompletableFuture及内置大量的工厂方法方便的实现异步接口，并结合Stream技术实现并发/并行编程，CompletableFuture实现的异步操作都是单任务操作。CompletableFuture类实现了CompletionStage和Future两个接口，一方面对传统的Future接口进行了增强，上面介绍的主要就是集中这个方面。下面就重点看下CompletionStage这个接口，它将流式思想引入到了并发/并行编程，让并发/并行编程具有了类似Stream的流水式操作的强大和灵活性，同时也降低了并发/并行编程的复杂性，这才是Java 8和之前并发/并行编程的一个本质区别。 需求描述： 1、任务1获取商品价格，任务2获取货币汇率，然后将任务1获取的商品价格*任务2获取的货币汇率相乘即可得到最终结果 2、任务1和任务2相互之间不存在依赖关系，所以任务1和任务2可以同时并行执行 3、任务1和任务2结果都出来后，才执行它们结果相乘得到最终结果Future&lt;Double&gt; futurePriceInUSD = CompletableFuture.supplyAsync(() -&gt; shop.getPrice(product)) .thenCombine(CompletableFuture.supplyAsync(() -&gt; exchangeService.getRate(Money.EUR, Money.USD)) , (price, rate) -&gt; price * rate);需求描述： 1、任务1是将数据写入数据库 2、任务2是将数据推送到第三方接口 3、任务1和任务2没有任何依赖关系，可以并行执行，通过join保证这两个任务都执行完成后才继续向下执行，如果不需要也可以不使用join进行阻塞CompletableFuture.runAsync(() -&gt; writeDb(alarmData), cachedThreadPool)//写入DB .runAsync(() -&gt; transferParallel(alarmData, uidSet), cachedThreadPool)//传送到推送服务器 .join();//阻塞直到上面2个任务执行完毕需求描述： 1、task1和task2同时并行执行，哪个执行快使用哪个的计算结果String result = CompletableFuture.supplyAsync(() -&gt; doTask1()) .applyToEither(CompletableFuture.supplyAsync(() -&gt; doTask2()), s -&gt; s) .join();CompletableFuture类提供了将两个CompletableFuture建立联系功能，通过迭代方式，可以让更多个CompletableFuture建立起关系，构建出更加复杂的业务逻辑，这就形成了类似Stream流式处理功能，只不过它内部元素不再是数据，而是一个个异步任务，而且通过内部抽象出来的语义接口，可以灵活实现这些异步任务间的依赖关系等，这就是流式编程模型的哲学：让程序只关注一个目标，并尽可能把它做好，让程序能够互相协同工作完成复杂任务。这才是真正体现CompletableFuture版本实现所具备的巨大优势，用CompletableFuture在代码简洁性、可读性上带来的巨大提升，提高了开发人员进行并发/并行编程的积极性。 CompletableFuture类提供的接口方法还是比较多的，但是这些接口原理上大致相同，理解上也不复杂，关键是要理解这些思想背后的逻辑及它的优势。 Fork/JoinForkJoin框架是在Java 7中引入的，即分支/合并框架，Stream并行流就是依赖ForkJoin将一个操作切分为多个子操作，在多个不同的核上并行地执行这些子操作，所以还是有必要简单认识下。 在ForkJoin框架出来之前，你要将任务拆解进行并发编程： 1、你得明确地把包含数据的数据结构分成若干子部分 2、你要给每个子部分分配一个独立的线程 3、你需要在恰当的时候对它们进行同步来避免不希望出现的竞争条件，等待所有线程完成，最后把这些部分结果合并起来 Java 7引入ForkJoin就是让这些操作更稳定、更不易出错，ForkJoin框架的原理：以递归方式将可以并行的任务拆分成更小的任务，然后将每个子任务的结果合并起来生成整体结果。它是ExecutorService接口的一个实现，它把子任务分配给线程池（称为ForkJoinPool）中的工作线程。 使用ForkJoin框架另一个好处就是：它实现了“工作窃取”机制，这种算法用于在池中的工作线程之间重新分配和平衡任务，直白的说就是：某个线程从其他队列里窃取任务来执行。假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，线程和队列一一对应，比如A线程负责处理A队列里的任务。但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。而在这时它们会访问同一个队列，所以为了减少窃取任务线程和被窃取任务线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。 工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182ForkJoin框架编程的模板伪代码大致如下： if (任务足够小或不可分) &#123; 顺序计算该任务 &#125; else &#123; 将任务分成两个子任务 递归调用本方法，拆分每个子任务，等待所有子任务完成 合并每个子任务的结果 &#125;你可能已经注意到，这只不过是著名的分递归治算法的并行版本而已。 案例如下：import java.util.concurrent.RecursiveTask;/** * @author 36410 * @Copyright © 2017 tiger Inc. All rights reserved. * @create 2017-12-11 10:27 * Description:你可能已经注意到，这只不过是著名的分治算法的并行版本而已 */public class ForkJoinSumCalculator extends RecursiveTask&lt;Long&gt; &#123; private final long[] numbers; private final int start; private final int end; public static final long THRESHOLD = 10_000; public ForkJoinSumCalculator(long[] numbers) &#123; this(numbers, 0, numbers.length); &#125; private ForkJoinSumCalculator(long[] numbers, int start, int end) &#123; this.numbers = numbers; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; int length = end - start; if (length &lt;= THRESHOLD) &#123; return computeSequentially(); &#125; /** * 对子任务调用fork方法可以把它排进ForkJoinPool。同时对左边和右边的子任务调用它似乎很自然，但这样做的效率要比直接对其中一个调用compute低。 * 这样做你可以为其中一个子任务重用同一线程，从而避免在线程池中多分配一个任务造成的开销。 * * fork()：放入到ForkJoinPool线程池中执行，compute()重用当前线程执行任务 */ ForkJoinSumCalculator leftTask = new ForkJoinSumCalculator(numbers, start, start + length / 2); leftTask.fork(); ForkJoinSumCalculator rightTask = new ForkJoinSumCalculator(numbers, start + length / 2, end); Long rightResult = rightTask.compute(); Long leftResult = leftTask.join(); return leftResult + rightResult; &#125; private long computeSequentially() &#123; long sum = 0; for (int i = start; i &lt; end; i++) &#123; sum += numbers[i]; &#125; return sum; &#125;&#125;/** * @author 36410 * @Copyright © 2017 tiger Inc. All rights reserved. * @create 2017-12-11 10:30 * Description:Runtime.availableProcessors的返回值来决定线程池使用的线程数。请注意availableProcessors方法虽然看起来是处理器， 但它实际上返回的是可用内核的数量，包括超线程生成的虚拟内核 */public class Main &#123; public static long forkJoinSum(long n) &#123; long[] numbers = LongStream.rangeClosed(1, n).toArray(); ForkJoinTask&lt;Long&gt; task = new ForkJoinSumCalculator(numbers); return new ForkJoinPool().invoke(task); &#125;&#125; 总结Java自上世纪90年代出现，到如今已20多年历史，这期间开发生态已发生了很大的变化，一方面，多核时代让并发/并行编程成为一个必须面临解决的问题；另一方面，非结构化、半结构化数据的大量涌现导致数据处理功能由传统的数据库处理移植到程序开发中需要解决的问题。而这两种潮流的转变都能通过使用函数式编程非常轻松地得到支持，Java 8及时的引入lambda、Stream、CompletableFuture等，将流式数据计算的思想带入了Java 8中，可以说是完成了一次华丽的转型。 到这里为止，对Java 8总结基本要完结了，前面讲了那么多，其核心思想我这里归纳成如下几点： 1、lambda函数式编程是对流式编程思想的一个很好解决方案，反过来，正是流式计算模型的兴起带动了lambda函数式编程近几年的风靡 2、Stream是对流式思想和lambda函数式编程结合进行的一次极佳实践，本质上是将一系列lambda表达式运用流式思想进行组装成一条数据处理链，以完成复杂的任务；同时，Stream带来的声明式编程方式让开发者通过类似描述性语言就可以实现复杂业务逻辑开发，极大的降低了编程的复杂性；总之，Stream让传统的面向“存储”的集合具有了面向“计算”的能力 3、CompletableFuture完成了将并发/并行编程和流式思想相结合的重要创新，再结合lambda函数式编程，很好的解决了并发/并行编程的复杂性导致开发积极性不高、后期维护难等问题，现在通过CompletableFuture可以简洁的实现灵活强大的并发/并行编程，为并发/并行编程成为开发常态提供了强大的技术支持手段 4、一方面，CPU进入多核时代与现有的并发/并行编程不足；另一方面，海量的半结构化、非结构化数据的出现与传统的面向数据库编程(通过关系型数据库提供的丰富的函数完成各种业务数据处理)冲突，Java 8通过流式思想和lambda函数式编程的引入都得以很好的解决了这两个问题 5、总之，Java 8是历年来改变最大一次，但其本质上要实现的目标是将流式计算模型引入，lambda表达式只不过是对这种模型引入提供了一种优雅的解决方案 本文重点不是教你怎么使用lambda、Stream、CompletableFuture等这些Java 8中的新特性，而是希望你能看清这些改变的意义及背后的思想是什么。不要再仅仅认为：这些新特性只是一种很酷炫的技术，也仅仅只是让开发中的一些代码变得简洁，仅此而已。我希望：当你深入了解这些新特性背后的哲学、思想后，应该认识到这就是一种潮流，一种真实需求的体现。 最后用我一个切身感受结束：没有使用Lamdba之前，你可能永远都不想使用它，因为怪异的语法与现有的面向对象编程格格不入，但是当你开始熟悉并深入使用它后，你会迷恋上它。]]></content>
      <categories>
        <category>计算模型</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>计算模型</tag>
        <tag>Java8</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Disruptor之概览]]></title>
    <url>%2F2018%2F01%2F26%2FDisruptor%E4%B9%8B%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Diruptor概述“多核危机”驱动了并发编程的复兴，然后并发编程和一般的系统相比，复杂性有个很大梯度的上升。多线程开发很大困难在于：多个线程间存在依赖关系时，如何进行协调。依赖一方面是执行顺序的依赖，如某个线程执行需要依赖其他线程执行或其它线程的某些阶段执行结果，Java为我们提供的解决方案是：wait/notify、lock/condition、join、yield、Semaphore、CountDownLatch、CyclicBarrier以及JDK7新增的一个Phaser等；数据依赖主要是多个线程对同一资源并发修改导致的数据状态不一致问题，Java中主要依靠Lock和CAS两种方案，也就是我们熟知的悲观锁、乐观锁。 然而，当你在并发编程方面慢慢有些经验并开始在项目中使用时，你会发现仅仅依赖JDK提供的上面所说开发工具类是远远不够的， JDK提供的工具类都只能解决一个个功能“点”的问题。并发编程复杂性一个体现就是：多个顺序执行流在多核CPU中同时并行执行与我们已经习惯的单个数据顺序流执行的方式产生了很大的冲突。 好比：现在你开车从A地到B地去，传统的开发模式就像从A地到B地之间只存在一条公路，你只需要延着这个公路一直开下去就可以达到B地；假如经过多年发展，现在A地到B地横起有10条公路，纵起有10条公路，它们之间相互交叉形成错综复杂的公路网，你再开车从A地到B地就会存在太多的选择，可能从东南西北任何方向出发最终都能到达B地。这就体现了并发编程和传统编程复杂性的对比：传统编程由于只存在一个顺序执行流，可以很好的预判程序的执行流程；而并发编程存在太多的顺序执行流导致很难准确的预判出它们真正的执行流程，一旦出现问题也很难排查，就好比上面的例子第二种情况，你很难预判你开车的真正路线，而且可能存在每次路线都不一样情况。 我认为一个并发编程项目好坏其中一个关键核心就是：项目的整体结构是否清晰。很简单的一个例子，调用notify()方法唤醒挂起在指定对象上的休眠线程，如果没有一个清晰简单的架构设计，可能会导致在该对象上进行休眠的对象散落到系统中各处代码上，很难把控具体唤醒的是哪个线程从而与你的业务逻辑发生偏差导致bug的出现。当然，项目结构清晰在传统编程中也是非常看重的，只有结构清晰的架构才会让人易于理解，同时和他人沟通探讨时方便描述，但是在并发编程中这点尤为重要，因为并发编程的复杂性更高，没有一个清晰的结构设计，你可能经过大量测试修改暂时做出了一个看似没有bug的项目，但是后期需求变更或者是其他人来维护这个项目时，很难下手导致后期会引入大量的bug，而且不利于项目功能的扩展。 常用的并发编程使用的模型有并行模型、流水线模型、生产者/消费者模型、Actor模型等，采用模型设计一方面是因为这些模型都是大牛们经过长时间实际生产经验的积累总结出的并发编程方面一些好的解决方案；另一方面，采用模型设计可以解决相关人员之间沟通信息不对等问题，降低沟通学习成本。 并行模型是JDK8中Stream所采用的实现并发编程的方式，并行模型非常简单，就是为每个任务分配一个线程直到该任务执行结束，示意图如下： 并行模型太过简单导致对任务的精细化控制不足，一个任务可能会被分解为多个阶段，而每个阶段的子任务特性可能差别很大，这时并行模型就无能为力了。并行模型只适合于CPU密集型且任务中不含IO阻塞等情况的任务。这时，就演进出流水线模型，示意图如下： 流水线模型在实际的并发编程中使用比较常见，我们所说的Pipeline设计模型、Netty框架等都是这一思想的体现。 生产者/消费者模型在并发编程中也是使用频度非常高的一个模型，生产者/消费者模型可以很容易地将生产和消费进行解耦，优化系统整体结构，并且由于存在缓冲区，可以缓解两端性能不匹配的问题。 Actor模型其典型代表就是Akka，基于Akka可以轻松实现一个分布式异步数据处理集群系统，非常强大，后期我们有机会可以再深入讨论下Akka。 好了，说了这么多，终于要开始正题：Disruptor，官方宣传基于该框架构建的系统单线程可以支撑每秒处理600万订单，此框架真乃惊为天人。Disruptor在生产者/消费者模型上获得尽量高的吞吐量和尽量低的延迟，其目标就是在性能优化方面做到极致。国内国外都存在大量的知名项目在广泛使用，比如我们所熟知的strom底层就依赖Disruptor的实现，其在并发、缓存区、生产者/消费者模型、事务处理等方面都存在一些性能优秀的方案，因此是非常值得深入研究的。 生产者/消费者模型生产者/消费者模型在编程中使用频度非常高的一个模型，生产者/消费者模型可以很容易地将生产和消费进行解耦，优化系统整体结构，并且由于存在缓冲区，可以缓解两端性能不匹配的问题。生产者/消费者和我们所熟悉的设计模式中的观察者模型很相似，生产者类似于被观察者，消费者类似于观察者，被观察者的任何变动都以事件的方式通知到观察者；同理，生产者生产的数据都要传递给消费者最终都要被消费者处理。 一般项目开发中，我们可以使用JDK提供的阻塞队列BlockingQueue很简单的实现一个生产者/消费者模型，其中生产者线程负责提交需求，消费者线程负责处理任务，二者之间通过共享内存缓冲区进行通信。 BlockingQueue实现类主要有两个：ArrayBlockingQueue和LinkedBlockingQueue，底层实现一个是基于数组的，一个是基于链表的，这种实现方式的差异导致了它们使用场景的不一样。在生产者/消费者模型中的缓存设计上肯定优先使用ArrayBlockingQueue，但是查看ArrayBlockingQueue底层源码会发现，读写操作通过重入锁实现同步，而且读写操作使用的是同一把锁，并没有实现读写锁分离；另外，锁本身的成本还是比较高的，锁容易导致线程上下文频繁的发生切换，了解CPU核存储硬件架构的可能会知道，每核CPU都会存在一个独享的高速缓存L1，假如线程切换到其它CPU上执行会导致之前CPU高速缓存L1中的数据不能再被使用，降低了高速缓存使用效率。因此，在高并发场景下，性能不是很优越。 12345678910111213141516171819202122232425//向Queue中写入数据public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly();//可中断方式获取锁，实现同步 try &#123; while (count == items.length) notFull.await(); insert(e); &#125; finally &#123; lock.unlock(); &#125;&#125;//从Queue中取出数据public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly();//可中断方式获取锁，实现同步 try &#123; while (count == 0) notEmpty.await(); return dequeue(); &#125; finally &#123; lock.unlock(); &#125;&#125; Disruptor消息生产模型 Producer生产出一个消息事件Event，需要放入到RingBuffer中，流程大致如下： ​ 1、首先调用Sequencer.next()方法，获取RingBuffer上可用的序号用于将新生成的消息事件放入； ​ 2、Sequencer首先对nextValue+1代表当前需要申请的RingBuffer序号(nextValue标记了之前已经申请过的序号,nextValue+1就是下一个可申请的序号)，但是nextValue+1指向的RingBuffer槽位存放的消息可能并没有被消费，如果直接返回这个序号给生产者，就会导致生产一方将该槽位的消息事件重新填充覆盖导致之前数据丢失，这里就需要一个判断：判断申请的RingBuffer序号代表的槽位之前的消息事件是否已被消费，判断逻辑如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public long next(int n) &#123; if (n &lt; 1) //n表示此次生产者期望获取多少个序号，通常是1 &#123; throw new IllegalArgumentException("n must be &gt; 0"); &#125; long nextValue = this.nextValue; //这里n一般是1，代表申请1个可用槽位，nextValue+n就代表了期望申请的可用槽位序号 long nextSequence = nextValue + n; //减掉RingBuffer的bufferSize值，用于判断是否出现‘绕圈覆盖’ long wrapPoint = nextSequence - bufferSize; //cachedValue缓存之前获取的最慢消费者消费到的槽位序号 long cachedGatingSequence = this.cachedValue; //如果申请槽位序号-bufferSize比最慢消费者序号还大，代表生产者绕了一圈后又追赶上了消费者，这时候就不能继续生产了，否则把消费者还没消费的消息事件覆盖 if (wrapPoint &gt; cachedGatingSequence || cachedGatingSequence &gt; nextValue) &#123; /** cursor代表当前已经生产完成的序号，了解多线程可见性可能会知道： 1、CPU和内存间速度不匹配，硬件架构上一般会在内存和CPU间还会存在L1、L2、L3三级缓存 2、特别是L1高速缓存是CPU间相互独立不能共享的，线程操作可以看着基于L1缓存进行操作，就会导致线程间修改不会立即被其它线程感知，只有L1缓存的修改写入到主存然后其它线程将主存修改刷新到自己的L1缓存，这时线程1的修改才会被其它线程感知到 3、线程修改对其它线程不能立即可见特别是在高并发下可能会带来些问题，JAVA中使用volatile可以解决可见性问题 4、这里就是采用UNSAFE.putLongVolatile()插入一个StoreLoad内存屏障，具体可见JMM模型，主要保证cursor的真实值对所有的消费线程可见，避免不可见下消费线程无法消费问题 */ cursor.setVolatile(nextValue); long minSequence; //Util.getMinimumSequence(gatingSequences, nextValue)获取当前时刻所有消费线程中，消费最慢的序号 //上面说过cachedValue是缓存的消费者最慢的序号 //这样做目的：每次都去获取真实的最慢消费线程序号比较浪费资源，而是获取一批可用序号后，生产者只有使用完后，才继续获取当前最慢消费线程最小序号，重新获取最新资源 while (wrapPoint &gt; (minSequence = Util.getMinimumSequence(gatingSequences, nextValue))) &#123; //如果获取最新最慢消费线程最小序号后，依然没有可用资源，做两件事： // 1、唤醒waitStrategy上所有休眠线程，这里即是消费线程(避免因消费线程休眠而无法消费消息事件导致生产线程一直获取不到资源情况) // 2、自旋休眠1纳秒 //可以看到，next()方法是一个阻塞接口，如果一直获取不到可用资源，就会一直阻塞在这里 waitStrategy.signalAllWhenBlocking(); LockSupport.parkNanos(1L); &#125; //有可用资源时，将当前最慢消费线程序号缓存到cachedValue中，下次再申请时就可不必再进入if块中获取真实的最慢消费线程序号，只有这次获取到的被生产者使用完才会继续进入if块 this.cachedValue = minSequence; &#125; //申请成功，将nextValue重新设置，下次再申请时继续在该值基础上申请 this.nextValue = nextSequence; //返回申请到RingBuffer序号 return nextSequence; &#125; ​ 3、申请到可用序号后，提取RingBuffer中该序号中的Event，并重置Event状态为当前最新事件状态 ​ 4、重置完成后，调用Sequencer.publish()提交序号，提交序号主要就是修改cursor值，cursor标记已经生产完成序号，这样消费线程就可以来消费事件了 12345678@Overridepublic void publish(long sequence)&#123; //修改cursor序号，消费者就可以进行消费 cursor.set(sequence); //唤醒消费线程，比如消费线程消息到无可用消息时可能会进入休眠状态，当放入新消息就需要唤醒休眠的消费线程 waitStrategy.signalAllWhenBlocking();&#125; 总结：消息事件生产主要包含三个步骤： ​ 1、申请序号：表示从RingBuffer上获取可用的资源 ​ 2、填充事件：表示获取到RingBuffer上可用资源后，将新事件放入到该资源对应的槽位上 ​ 3、提交序号：表示第二部新事件放入到RingBuffer槽位全部完成，提交序号可供消费线程开始消费 Disruptor消息处理模型 消息处理端需要从RingBuffer中提取可用的消息事件，并注入到用户的业务逻辑中进行处理，流程大致如下： ​ 1、消费端核心类是EventProcessor，它实现了Runnable接口，Disruptor在启动的时候会将所有注册上来的EventProcessor提交到线程池中执行，因此，一个EventProcessor可以看着一个独立的线程流用于处理RingBuffer上的数据 ​ 2、EventProcessor通过调用SequenceBarrier.waitFor()方法获取可用消息事件的序号，其实SequenceBarrier内部还是调用WaitStrategy.waitFor()方法，WaitStrategy等待策略主要封装如果获取消息时没有可用消息时如何处理的逻辑信息，是自旋、休眠、直接返回等，不同场景需要使用不同策略才能实现最佳的性能 12345678910111213141516171819202122232425262728293031323334ProcessingSequenceBarrier： WaitStrategy waitStrategy; Sequence dependentSequence; boolean alerted = false; Sequence cursorSequence;//可供消费消息的sequence Sequencer sequencer;ProcessingSequenceBarrier中核心方法只有一个：waitFor(long sequence)，传入希望消费得到起始序号，返回值代表可用于消费处理的序号，一般返回可用序号&gt;=sequence，但也不一定，具体看WaitStrategy实现/** * 总结： * 1、sequence：EventProcessor传入的需要进行消费的起始sequence * 2、这里并不保证返回值availableSequence一定等于given sequence，他们的大小关系取决于采用的WaitStrategy * a.YieldingWaitStrategy在自旋100次尝试后，会直接返回dependentSequence的最小seq，这时并不保证返回值&gt;=given sequence * b.BlockingWaitStrategy则会阻塞等待given sequence可用为止，可用并不是说availableSequence == given sequence，而应当是指 &gt;= * c.SleepingWaitStrategy:首选会自旋100次，然后执行100次Thread.yield()，还是不行则LockSupport.parkNanos(1L)直到availableSequence &gt;= given sequence */ @Override public long waitFor(final long sequence) throws AlertException, InterruptedException, TimeoutException &#123; checkAlert(); //调用WaitStrategy获取RingBuffer上可用消息序号，无可消费消息是该接口可能会阻塞，具体逻辑由WaitStrategy实现 long availableSequence = waitStrategy.waitFor(sequence, cursorSequence, dependentSequence, this); if (availableSequence &lt; sequence) &#123; return availableSequence; &#125; //获取消费者可以消费的最大的可用序号，支持批处理效应，提升处理效率。 //当availableSequence &gt; sequence时，需要遍历 sequence --&gt; availableSequence，找到最前一个准备就绪，可以被消费的event对应的seq。 //最小值为：sequence-1 return sequencer.getHighestPublishedSequence(sequence, availableSequence); &#125; ​ 3、通过waitFor()返回的是一批可用消息的序号，比如申请消费7好槽位，waitFor()返回的可能是8表示从6到8这一批数据都已生产完毕可以进行消费 ​ 4、EventProcessor按照顺序从RingBuffer中取出消息事件，然后调用EventHandler.onEvent()触发用户的业务逻辑进行消息处理 1234567891011121314151617181920212223while (true) &#123; try &#123; //读取可消费消息序号 final long availableSequence = sequenceBarrier.waitFor(nextSequence); if (batchStartAware != null) &#123; batchStartAware.onBatchStart(availableSequence - nextSequence + 1); &#125; while (nextSequence &lt;= availableSequence) &#123; //循环提取所有可供消费的消息事件 event = dataProvider.get(nextSequence); //将提取的消息事件注入到封装用户业务逻辑的Handler中 eventHandler.onEvent(event, nextSequence, nextSequence == availableSequence); nextSequence++; &#125; sequence.set(availableSequence); &#125; &#125;&#125; ​ 5、当这批次的消息处理完成后，继续重复上面操作调用waitFor()继续获取可用的消息序号，周而复始 好了，这节主要对Disruptor的生产模型和消费模型进行了一个简单的介绍，后面会逐渐对Disruptor涉及到的每个核心组件进行分析，了解它们优秀的设计思想。]]></content>
      <categories>
        <category>Disruptor</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>计算模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时区总结]]></title>
    <url>%2F2018%2F01%2F20%2F%E6%97%B6%E5%8C%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近工作上涉及到些时区概念，在同事交流过程中顺便总结了下。 时区下面用如下系统结构介绍下时区概念： 1、现在前端WEB需要获取到当前时间:new Date() ，new Date()创建时间代表当前时刻，是不涉及到任何时区问题的，Date类型的API中也没有任何和时区进行关联的方法，Date可以代表对时间戳long的一个封装，表示一个瞬时时刻，Date.getTime()方法就可以获取到当前封装的时间戳值，同理也可以通过new Date(时间戳)来创建一个时间，比如当前时间戳：1514966524591L 2、现在要把这个获取到的new Date()传递到服务A中，假如使用常用到的REST接口进行传递，我们知道REST是通过Json字符串传递参数的，这就涉及到Date到字符串转换的问题了，要用到SimpleDateFormat时间格式化类： 1234SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");System.out.println(dateFormat.format(date));输出如下：2018-01-03 16:02:04.591 由于时间格式化时没有指定时区，默认采用当前系统时区，所以这个输出就代表：北京时间(东八区)2018-01-03 16:02:04.591。当然现在框架都是集成了Json转换工具类，你不需要像我这里自己创建SimpleDateFormat然后指定时区，但原理都是一样的，一般框架将Date转成Json对象都是会提供设置时区的接口的。 如果和服务A约定使用UTF零时区而不是使用本地时区呢，因为使用本地时区存可能会存在问题，假如WEB服务部署到东九区而不是东八区，这样使用SimpleDateFormat解析new Date()使用的是东九区时区，服务A收到了可能依然会认为是东八区，这样服务A解析传递过来的时间字符串就会存在问题。WEB服务和服务A就可以约定传递使用UTF时区，保证就不会出现问题，但是WEB在将Date转成String的时候需要指定UTF时区，不能再使用默认的系统本地时区了，如下： 12345SimpleDateFormat dateFormat2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");dateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));//指定为UTF时区System.out.println(dateFormat.format(date));输出如下：2018-01-03 08:02:04.591 发现比之前的2018-01-03 16:02:04.591少了8个小时，这就体现了同一个时间，在不同时区代表的具体时间，即几月几日，几点几分是不一样的，但是东八区的2018-01-03 16:02:04.591和UTF时区的2018-01-03 08:02:04.591代表的是同一个时间，即同一时刻。 3、服务A接收到2018-01-03 16:02:04.591这个代表时间的字符串，需要将其转成成Date类型，它就需要知道这个时间字符串到底代表那个时区的，当然如果服务A本身就是运行在东八区就不需要指定时区(默认和系统时区保持一致)，否则解析的时候就需要指定具体的时区： 1234567String timeStr = "2018-01-03 16:02:04.591";SimpleDateFormat dateFormat2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");dateFormat2.setTimeZone(TimeZone.getTimeZone("GMT+8:00"));Date date2 = dateFormat2.parse(timeStr);System.out.println(date2.getTime());输出：1514966524591(和WEB传入的Date的时间戳一致，即传递过程正常) 如果WEB传递过来的是UTF时区时间：2018-01-03 08:02:04.591，服务A解析如下：1234567String timeStr = "2018-01-03 08:02:04.591";SimpleDateFormat dateFormat3 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS");dateFormat3.setTimeZone(TimeZone.getTimeZone("GMT"));//指定用UTF时区解析时间Date date3 = dateFormat3.parse(timeStr);System.out.println(date3.getTime());输出：1514966524591(和WEB传入的Date的时间戳一致，即传递过程正常) 总结：如果系统间时区不一致，或者约定传递时间用UTF时间格式，就要保证两个方面： 1、传递方在将Date解析成字符串时指定时区为UTF 2、接收方在接收到时间字符串进行解析成Date类型时，也需要指定时区为UTF 3、如果不指定都会采用系统默认时区，可能就会存在问题 4、服务A正常解析出WEB传递过来的时间后，写入到数据库表中的一个字段中，如果字段类型是datetime，插入数据是不受任何时区影响的(包括服务本地时区、MySQL运行服务器时区及MySQL自己时区)，这就是Date代表的时间至少时间戳的封装，相当于一个long型，时间戳是没有时区概念的。 123456789查看数据库时区：show variables like "%time_zone%";+------------------+--------+| Variable_name | Value |+------------------+--------+| system_time_zone | CST || time_zone | SYSTEM |+------------------+--------+2 rows in set (0.00 sec) time_zone说明mysql使用system的时区，system_time_zone说明system使用CST时区，CST时区在这里就代表的是北京东八区时区、 但是这里要注意下：你使用select查询的时候，其实是存在数据库中Date类型到字符串转换的过程，因为你使用select查询看到的时间一般都是“2017-10-1 12:00:00”等这种格式，这就是数据库时区的作用，同理，如果数据库字段是datetime，你insert时指定的是“2017-10-1 12:00:00”等这种数据，数据库也是会存在上面说的字符串转成Date要涉及到时区，这里使用的也是数据库时区，其实更上面介绍的Date传递原理是一致的。如果你把数据库时区修改了，select查询出来同样的数据，展示的时间字符串是不一样的，就是这个道理。 5、服务B从数据库查询，查询出来的是一个Date类型，这里是不会涉及到任何时区问题。可能经常会出现下面一种情况： 12341、客户端1从数据库查询该数据看到的是2018-01-03 16:02:04.591，因为没有指定时区默认使用的是本地时区，假设为东八区2、客户端2从数据库查询拿到同样的数据，但是输出的确是2018-01-03 08:02:04.591，因为客户端本地系统时区为UTF时区3、虽然客户端1和客户端2看到像是不同的时间，但是它们拿到的是同一份数据，指向的时间也是同一个时刻的时间，即这一个时间时刻在不同时区有不同的表示，但是代表的时间都是同一个时刻4、如果需要统一时区，需要向之前样式，转换时指定时区即可 ​ 总结计算机中的时间都是通过时间戳表示的，如程序中的Date类型、数据库中的Date类型，其内部就是对时间戳的一个封装，时间戳没有时区概念，所以在计算机中的时间是没有时区概念的。 那么为什么会有时区概念呢？时区存在主要是为了：自然时间表示时必须要有时区才有意义，否则时间是没有意义的。自然时间就是现实生活中人们所认识的时间，计算机中的时间是时间戳，在现实生活中肯定是不方便理解的，现实生活中我们还是更倾向使用”2018-01-20 18:00:00”这种自然时间，但是这个时间表示是有问题的：到底是哪个时区的1月20号18点呢？ 时间的概念更倾向于时刻，它是全世界唯一的，比如现在这个时刻东八区北京时间是：2018-1-25 15:30:30，其对应的时间戳是1516865430404，但是同样这刻在零时区时间是：2018-1-25 07:30:30，但是它的时间戳依然是1516865430404，也就是说，计算机中的时间戳是一个时刻的概念，不存在时区区别，但是当这个时刻表示为自然时间时就存在不同时区表示方法不一样了。 综上来看，我们在程序开发中，当存在”2018-01-20 18:00:00”解析为Date类型，或Date类型格式化为”2018-01-20 18:00:00”，即计算机时间和自然时间相互转换时，就一定要注意时区，这是因为当出现”2018-01-20 18:00:00”必须带有时区才有意义。 在平时开发中，如果多系统间需要传递时间，最近实践是采用时间戳传递而不是”2018-01-20 18:00:00”这种时间格式传递，原因如下：​ 1、时间戳占用的字节大小比2018-01-03 16:02:04.591小很多，在系统间传递网络带宽更有优势；​ 2、当然最主要的还是：时间戳不存在时区概念，不需要进行转换，而2018-01-03 16:02:04.591格式在传递时进行转换非常容易引起混乱导致错误​ 3、数据库中的时间类型采用long型时间戳，效率更高]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>其它</category>
      </categories>
      <tags>
        <tag>其它</tag>
      </tags>
  </entry>
</search>
